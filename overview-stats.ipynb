{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "696654e4",
   "metadata": {},
   "source": [
    "# OpenAIRE Graph University Overview\n",
    "\n",
    "This notebook fetches summary statistics per university using the [OpenAIRE Graph API](https://graph.openaire.eu/docs/apis/graph-api/). For every listed university we compare three perspectives:\n",
    "\n",
    "- **A. Publications affiliated to the university** – filter on the OpenAIRE OpenORG identifier.\n",
    "- **B. Publications collected by the main (CRIS) data source** – filter on the CRS/data source identifier.\n",
    "- **C. Publications collected by the secondary repository** – filter on the repository identifier when available.\n",
    "\n",
    "For each perspective we retrieve counts for funding/projects, data sources, and research products split into publications, datasets, software, and other research outputs. The notebook separates the data collection steps so that each can be re-run independently when debugging or iterating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca1ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages (if not already installed)\n",
    "\n",
    "!pip install pandas matplotlib openpyxl python-dotenv requests tqdm pyarrow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86519c7f",
   "metadata": {},
   "source": [
    "## 1. Imports and reused constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, Optional, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.float_format\", lambda value: f\"{value:,.0f}\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "CLIENT_ID = os.getenv(\"CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"CLIENT_SECRET\")\n",
    "\n",
    "if not CLIENT_ID or not CLIENT_SECRET:\n",
    "    raise RuntimeError(\n",
    "        \"Missing OpenAIRE credentials. Set CLIENT_ID and CLIENT_SECRET in the environment.\"\n",
    "    )\n",
    "\n",
    "BASE_URL = \"https://api.openaire.eu/graph\"\n",
    "TOKEN_URL = \"https://aai.openaire.eu/oidc/token\"\n",
    "API_USER_AGENT = \"OpenAIRE-tools overview-stats notebook\"\n",
    "API_PAUSE_SECONDS = 0.1  # throttle requests a bit to stay within rate limits\n",
    "TOKEN_REFRESH_BUFFER = 60  # refresh the token one minute before expiration\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "IMG_DIR = Path(\"img\")\n",
    "IMG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "PRODUCT_TYPE_LABELS = {\n",
    "    \"publication\": \"Publications\",\n",
    "    \"dataset\": \"Research data\",\n",
    "    \"software\": \"Research software\",\n",
    "    \"other\": \"Other research products\",\n",
    "}\n",
    "\n",
    "METRIC_ORDER = [\n",
    "    \"Funding / Projects\",\n",
    "    \"Data sources\",\n",
    "    *PRODUCT_TYPE_LABELS.values(),\n",
    "]\n",
    "\n",
    "COMPARISON_LONG_PATH = DATA_DIR / \"comparison_long.csv\"\n",
    "COMPARISON_PIVOT_PATH = DATA_DIR / \"comparison_pivot.csv\"\n",
    "\n",
    "_access_token: Optional[str] = None\n",
    "_access_token_expiry: float = 0.0\n",
    "\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e359a7",
   "metadata": {},
   "source": [
    "## 2. Parse the university reference table\n",
    "The raw table below mirrors the values supplied in the request. We reshape it into a structured list so that the rest of the notebook can iterate over the entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42836ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the NL organizations reference table\n",
    "# Download the latest NL organizations baseline from the Google Sheets URL\n",
    "\n",
    "# URL of the published Google Sheets NL organizations baseline\n",
    "nl_orgs_baseline_url = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vTDQiWDIaI1SZkPTMNCovicBhA-nQND1drXoUKvrG1O_Ga3hLDRvmQZao_TvNgmNQ/pub?output=xlsx\"\n",
    "# Local path to store the downloaded baseline\n",
    "NL_ORGS_BASELINE_PATH = DATA_DIR / \"nl_orgs_baseline.xlsx\"\n",
    "# Name of the sheet containing the NL organizations data\n",
    "NL_ORGS_SHEET_NAME = \"nl-orgs\"\n",
    "\n",
    "# Function to download the NL organizations baseline\n",
    "def download_nl_orgs_baseline(url: str, dest: Path) -> Path:\n",
    "    \"\"\"Download the latest NL organizations baseline and store it locally.\"\"\"\n",
    "    # Ensure the destination directory exists\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if dest.exists():\n",
    "        print(f\"Using existing baseline file: {dest}\")\n",
    "        return dest\n",
    "        \n",
    "    print(f\"Downloading baseline file from {url}\")\n",
    "    # Download the file\n",
    "    response = requests.get(url, timeout=120)\n",
    "    response.raise_for_status()\n",
    "    dest.write_bytes(response.content)\n",
    "    print(f\"Saved baseline file to: {dest}\")\n",
    "    return dest\n",
    "\n",
    "# Download the NL organizations baseline\n",
    "download_nl_orgs_baseline(nl_orgs_baseline_url, NL_ORGS_BASELINE_PATH)\n",
    "\n",
    "# Set the table path to the downloaded baseline\n",
    "TABLE_PATH = NL_ORGS_BASELINE_PATH\n",
    "\n",
    "# Load and parse the NL organizations reference table\n",
    "def load_university_table(path: Union[str, Path], sheet_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the NL organizations reference table and normalise empty values to blanks.\"\"\"\n",
    "    table_path = Path(path)\n",
    "    if not table_path.exists():\n",
    "        raise FileNotFoundError(f\"Reference table not found: {table_path}\")\n",
    "    excel_suffixes = {\".xlsx\", \".xls\"}\n",
    "    if table_path.suffix.lower() in excel_suffixes:\n",
    "        # Load the Excel file\n",
    "        df = pd.read_excel(table_path, sheet_name=sheet_name, dtype=str, keep_default_na=False)\n",
    "    else:\n",
    "        # Load as a TSV/CSV file\n",
    "        df = pd.read_csv(table_path, sep=\"\t\", dtype=str, keep_default_na=False)\n",
    "    return df.fillna(\"\")\n",
    "\n",
    "# Helper function to pick the first non-empty value from a list of columns\n",
    "def _pick(row: pd.Series, *columns: str) -> str:\n",
    "    for column in columns:\n",
    "        if column not in row:\n",
    "            continue\n",
    "        value = row[column]\n",
    "        if isinstance(value, str):\n",
    "            value = value.strip()\n",
    "        if value:\n",
    "            return value\n",
    "    return \"\"\n",
    "\n",
    "# Normalize ROR link\n",
    "def normalise_ror_link(value: str) -> str:\n",
    "    # Normalize ROR identifier to a full URL\n",
    "    value = (value or \"\").strip()\n",
    "    # If the value is empty after stripping, return an empty string\n",
    "    if not value:\n",
    "        return \"\"\n",
    "    # If the value already starts with \"http\", return it as is\n",
    "    if value.startswith(\"http\"):\n",
    "        return value\n",
    "    # If the value starts with \"ror.org\", prepend \"https://\"\n",
    "    if value.startswith(\"ror.org\"):\n",
    "        return f\"https://{value}\"\n",
    "    # Otherwise, assume it's a ROR ID and construct the full URL\n",
    "    value = value.strip().strip(\"/\")\n",
    "    # If the value is empty after stripping, return an empty string\n",
    "    if not value:\n",
    "        return \"\"\n",
    "    # Return the full ROR URL\n",
    "    return f\"https://ror.org/{value}\"\n",
    "\n",
    "# Extract ROR ID from a value\n",
    "def extract_ror_id(value: str) -> str:\n",
    "    value = (value or \"\").strip()\n",
    "    if not value:\n",
    "        return \"\"\n",
    "    if value.startswith(\"http\"):\n",
    "        value = value.rstrip(\"/\").split(\"/\")[-1]\n",
    "    return value\n",
    "\n",
    "# Parse university rows from the DataFrame\n",
    "def parse_university_rows(df: pd.DataFrame) -> list[dict[str, Optional[str]]]:\n",
    "    # Parse university rows from the DataFrame and extract relevant fields\n",
    "    parsed: list[dict[str, Optional[str]]] = []\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Determine the university name using multiple possible columns\n",
    "        name = _pick(row, \"full_name_in_English\", \"University\", \"organization_name\")\n",
    "        # Skip rows without a valid name\n",
    "        if not name:\n",
    "            continue\n",
    "        \n",
    "        # Extract and normalize ROR information\n",
    "        ror_id = extract_ror_id(_pick(row, \"ROR\"))\n",
    "        # Normalize ROR link\n",
    "        ror_link = normalise_ror_link(_pick(row, \"ROR_LINK\") or ror_id)\n",
    "        # If ROR link is available but ROR ID is not, extract ROR ID from the link\n",
    "        if ror_link and not ror_id:\n",
    "            # Extract ROR ID from the normalized ROR link\n",
    "            ror_id = extract_ror_id(ror_link)\n",
    "\n",
    "        # Construct the university entry\n",
    "        entry = {\n",
    "            \"name\": name,\n",
    "            \"acronym_EN\": _pick(row, \"acronym_EN\") or None,\n",
    "            \"acronym_AGG\": _pick(row, \"acronym_AGG\") or None,\n",
    "            \"grouping\": _pick(row, \"main_grouping\") or None,\n",
    "            \"ROR\": ror_id or None,\n",
    "            \"ROR_LINK\": ror_link or None,\n",
    "            \"OpenAIRE_ORG_ID\": _pick(row, \"OpenAIRE_ORG_ID\", \"OpenAIRE OpenORG ID\", \"OpenAIRE OpenORG ID LINK\", \"openorg_id\") or None,\n",
    "            \"main_datasource_id\": _pick(row, \"OpenAIRE Data Source ID (Main/CRIS)\", \"OpenAIRE Data Source ID (Main/CRIS) LINK\", \"main_datasource_id\") or None,\n",
    "            \"secondary_datasource_id\": _pick(row, \"OpenAIRE Data Source (Secondary/Repository)\", \"OpenAIRE Data Source (Secondary/Repository) LINK\", \"secondary_datasource_id\") or None,\n",
    "        }\n",
    "        # Special handling for OpenDOAR IDs\n",
    "        if entry[\"main_datasource_id\"] and entry[\"main_datasource_id\"].startswith(\"opendoar\"):\n",
    "            if not entry[\"secondary_datasource_id\"]:\n",
    "                entry[\"secondary_datasource_id\"] = entry[\"main_datasource_id\"]\n",
    "                entry[\"main_datasource_id\"] = None\n",
    "        # Append the parsed entry to the list\n",
    "        parsed.append(entry)\n",
    "\n",
    "    # Return the parsed university entries\n",
    "    return parsed\n",
    "\n",
    "# Load the NL organizations reference table and parse universities\n",
    "nl_orgs_df = load_university_table(TABLE_PATH, NL_ORGS_SHEET_NAME)\n",
    "# Parse universities from the loaded DataFrame\n",
    "universities = parse_university_rows(nl_orgs_df)\n",
    "# Display the number of parsed universities\n",
    "print(f\"Parsed {len(universities)} universities from the reference table (loaded from {TABLE_PATH}).\")\n",
    "# Convert the list of universities to a DataFrame for display\n",
    "universities_df = pd.DataFrame(universities)\n",
    "# Display the DataFrame of universities\n",
    "universities_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece3bd2d",
   "metadata": {},
   "source": [
    "## 3. Helper functions for the Graph API\n",
    "These functions wrap the REST requests and centralise filter construction per scenario. Each call prints nothing by default so we can reuse them freely in later cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce1e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the different scenarios for metrics collection.\n",
    "# Each scenario includes a key, label, ID field, and description.\n",
    "# For each scenario, we will build appropriate filters to query the OpenAIRE Graph API.\n",
    "# For example, we can collect metrics based on organization affiliation,\n",
    "# main data source, or secondary repository.\n",
    "# Each scenario will have its own set of filters.\n",
    "\n",
    "SCENARIO_DEFS = [\n",
    "    {\n",
    "        \"key\": \"organization\",\n",
    "        \"label\": \"A. OpenORG affiliation\",\n",
    "        \"id_field\": \"OpenAIRE_ORG_ID\",\n",
    "        \"description\": \"Publications affiliated to the university (OpenAIRE OpenORG ID)\",\n",
    "    },\n",
    "    {\n",
    "        \"key\": \"main_datasource\",\n",
    "        \"label\": \"B. Main/CRIS data source\",\n",
    "        \"id_field\": \"main_datasource_id\",\n",
    "        \"description\": \"Publications collected from the main CRIS data source\",\n",
    "    },\n",
    "    {\n",
    "        \"key\": \"secondary_datasource\",\n",
    "        \"label\": \"C. Secondary repository\",\n",
    "        \"id_field\": \"secondary_datasource_id\",\n",
    "        \"description\": \"Publications collected from the secondary / repository data source\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Define filter builders for each scenario.\n",
    "# Each builder function takes an entity ID and returns a dictionary of filters\n",
    "# for projects, data sources, and research products.\n",
    "#   - The filters are used to query the OpenAIRE Graph API for relevant metrics.\n",
    "\n",
    "FILTER_BUILDERS = {\n",
    "    \"organization\": lambda entity_id: {\n",
    "        \"projects\": {\"relOrganizationId\": entity_id},\n",
    "        \"dataSources\": {\"relOrganizationId\": entity_id},\n",
    "        \"researchProducts\": {\"relOrganizationId\": entity_id},\n",
    "    },\n",
    "    \"main_datasource\": lambda entity_id: {\n",
    "        \"projects\": {\"relCollectedFromDatasourceId\": entity_id},\n",
    "        \"dataSources\": {\"id\": entity_id},\n",
    "        \"researchProducts\": {\"relCollectedFromDatasourceId\": entity_id},\n",
    "    },\n",
    "    \"secondary_datasource\": lambda entity_id: {\n",
    "        \"projects\": {\"relCollectedFromDatasourceId\": entity_id},\n",
    "        \"dataSources\": {\"id\": entity_id},\n",
    "        \"researchProducts\": {\"relCollectedFromDatasourceId\": entity_id},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define an empty metrics dictionary for collecting results.\n",
    "EMPTY_METRICS = {metric: None for metric in METRIC_ORDER}\n",
    "\n",
    "# Obtain a cached OpenAIRE access token, refreshing it when needed.\n",
    "def obtain_access_token() -> str:\n",
    "    \"\"\"Return a cached OpenAIRE access token, refreshing it when needed.\"\"\"\n",
    "    global _access_token, _access_token_expiry\n",
    "    now = time.time()\n",
    "    if _access_token and now < _access_token_expiry:\n",
    "        return _access_token\n",
    "\n",
    "    response = requests.post(\n",
    "        TOKEN_URL,\n",
    "        data={\"grant_type\": \"client_credentials\"},\n",
    "        auth=(CLIENT_ID, CLIENT_SECRET),\n",
    "        headers={\"User-Agent\": API_USER_AGENT},\n",
    "        timeout=60,\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    payload = response.json()\n",
    "    token = payload.get(\"access_token\")\n",
    "    if not token:\n",
    "        raise RuntimeError(\"OpenAIRE token response did not include an access_token.\")\n",
    "    expires_in = int(payload.get(\"expires_in\", 3600))\n",
    "    _access_token = token\n",
    "    _access_token_expiry = now + max(expires_in - TOKEN_REFRESH_BUFFER, 0)\n",
    "    return _access_token\n",
    "\n",
    "# Invoke the OpenAIRE Graph API and return the decoded JSON payload.\n",
    "def call_graph_api(path: str, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Invoke the OpenAIRE Graph API and return the decoded JSON payload.\"\"\"\n",
    "    url = f\"{BASE_URL}{path}\"\n",
    "    effective_params = dict(params or {})\n",
    "    effective_params.setdefault(\"page\", 1)\n",
    "    effective_params.setdefault(\"pageSize\", 1)\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": API_USER_AGENT,\n",
    "        \"Authorization\": f\"Bearer {obtain_access_token()}\",\n",
    "    }\n",
    "\n",
    "    response = requests.get(\n",
    "        url,\n",
    "        params=effective_params,\n",
    "        headers=headers,\n",
    "        timeout=60,\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    time.sleep(API_PAUSE_SECONDS)\n",
    "    return response.json()\n",
    "\n",
    "# Retrieve the total number of matching records for the supplied endpoint.\n",
    "def fetch_num_found(path: str, params: Dict[str, Any]) -> Optional[int]:\n",
    "    \"\"\"Return the total number of matching records for the supplied endpoint.\"\"\"\n",
    "    payload = call_graph_api(path, params)\n",
    "    header = payload.get(\"header\", {})\n",
    "    num_found = header.get(\"numFound\")\n",
    "    return int(num_found) if num_found is not None else None\n",
    "\n",
    "# Build filters for the given scenario and entity ID.\n",
    "def build_filters(scenario_key: str, entity_id: str) -> Dict[str, Dict[str, Any]]:\n",
    "    builder = FILTER_BUILDERS[scenario_key]\n",
    "    return {name: dict(filters) for name, filters in builder(entity_id).items()}\n",
    "\n",
    "\n",
    "# Collect metrics for a given scenario and entity ID.\n",
    "def collect_metrics(scenario_key: str, entity_id: Optional[str]) -> Dict[str, Optional[int]]:\n",
    "    if not entity_id:\n",
    "        return deepcopy(EMPTY_METRICS)\n",
    "\n",
    "    filters = build_filters(scenario_key, entity_id)\n",
    "    results: Dict[str, Optional[int]] = {}\n",
    "\n",
    "    results[\"Funding / Projects\"] = fetch_num_found(\"/v1/projects\", filters[\"projects\"])\n",
    "    results[\"Data sources\"] = fetch_num_found(\"/v1/dataSources\", filters[\"dataSources\"])\n",
    "\n",
    "    for rp_type, label in PRODUCT_TYPE_LABELS.items():\n",
    "        rp_params = dict(filters[\"researchProducts\"], type=rp_type)\n",
    "        results[label] = fetch_num_found(\"/v2/researchProducts\", rp_params)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6634bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "# Cache OpenAIRE ID lookups to avoid redundant API calls\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def _fetch_openorg_id(ror_url: str) -> Optional[str]:\n",
    "    # Query the OpenAIRE Graph API for the organization with the given ROR URL\n",
    "    try:\n",
    "        payload = call_graph_api(\"/v1/organizations\", {\"pid\": ror_url})\n",
    "    except requests.RequestException as exc:\n",
    "        print(f\"Failed to fetch OpenAIRE ID for {ror_url}: {exc}\")\n",
    "        return None\n",
    "\n",
    "    # Extract the OpenAIRE organization ID from the response\n",
    "    results = payload.get(\"results\") or []\n",
    "    for item in results:\n",
    "        openorg_id = item.get(\"id\")\n",
    "        if openorg_id:\n",
    "            return openorg_id\n",
    "    return None\n",
    "\n",
    "# Look up the OpenAIRE organization identifier using a ROR ID or URL.\n",
    "\n",
    "def fetch_openorg_id_for_ror(ror_value: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"Look up the OpenAIRE organization identifier using a ROR ID or URL.\"\"\"\n",
    "    if not ror_value:\n",
    "        return None\n",
    "    # Normalize the ROR value to a URL\n",
    "    ror_url = normalise_ror_link(ror_value)\n",
    "    if not ror_url:\n",
    "        return None\n",
    "    # Look up the OpenAIRE organization ID using the normalized ROR URL\n",
    "    return _fetch_openorg_id(ror_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b919d9",
   "metadata": {},
   "source": [
    "## 4. Test for one university\n",
    "\n",
    "Run a quick test for the first university in the list to fetch the \n",
    "  OpenORG affiliation,\n",
    "  and the Number of: \n",
    "  Funding / Projects,\n",
    "  Data sources, \n",
    "  Products: Total,\n",
    "  Products: Publications, \n",
    "  Products: Research data, \n",
    "  Products: Research software,\n",
    "  Products: Other research products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: collect and display metrics for the first university in the list\n",
    "test_university = universities[0]\n",
    "print(f\"Testing metrics for: {test_university['name']}\")\n",
    "\n",
    "# Determine the OpenAIRE organization identifier\n",
    "identifier = test_university.get(\"OpenAIRE_ORG_ID\")\n",
    "if not identifier:\n",
    "    # Try to resolve it via the ROR ID/link\n",
    "    resolved = fetch_openorg_id_for_ror(test_university.get(\"ROR_LINK\") or test_university.get(\"ROR\"))\n",
    "    identifier = resolved\n",
    "\n",
    "# Collect and display metrics    \n",
    "print(f\"\\nOpenAIRE ID: {identifier}\")\n",
    "print(\"\\nMetrics:\")\n",
    "# Collect metrics for the organization\n",
    "metrics = collect_metrics(\"organization\", identifier)\n",
    "\n",
    "# Calculate total research products\n",
    "research_products_total = sum(\n",
    "    metrics.get(product_type, 0) or 0 \n",
    "    for product_type in [\"Publications\", \"Research data\", \"Research software\", \"Other research products\"]\n",
    ")\n",
    "print(\"Total Research Products:\", research_products_total)\n",
    "print(\"\\nBreakdown by type:\")\n",
    "\n",
    "# Display the collected metrics\n",
    "for metric, count in metrics.items():\n",
    "    print(f\"{metric}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b427e134",
   "metadata": {},
   "source": [
    "## 5. Collect metrics for all universities\n",
    "Loop through every organisation, ensure an OpenAIRE identifier is available, and add this information to new columns. and write to file nl_orgs_openaire.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc7ca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Enrich the NL organizations table with OpenAIRE IDs and metrics\n",
    "\n",
    "# Copy the university table so we can enrich it without mutating the original\n",
    "enriched_df = universities_df.copy()\n",
    "\n",
    "# Ensure all metric columns exist in the DataFrame\n",
    "metric_columns = [\n",
    "    \"Data sources\",\n",
    "    \"Total Research Products\",\n",
    "    \"Publications\",\n",
    "    \"Research data\",\n",
    "    \"Research software\",\n",
    "    \"Other research products\",\n",
    "]\n",
    "for column in metric_columns:\n",
    "    if column not in enriched_df.columns:\n",
    "        enriched_df[column] = pd.NA\n",
    "\n",
    "# Define output paths\n",
    "output_path = DATA_DIR / \"nl_orgs_openaire.xlsx\"\n",
    "checkpoint_path = DATA_DIR / \"nl_orgs_openaire.tmp.xlsx\"\n",
    "\n",
    "# Define parameters for enrichment process\n",
    "save_every = 5 # save every N rows\n",
    "max_workers = 6 # number of concurrent workers\n",
    "\n",
    "# Function to enrich a single row with OpenAIRE ID and metrics\n",
    "def enrich_row(idx: int) -> tuple[int, dict[str, Any]]:\n",
    "    # Enrich a single row with OpenAIRE ID and metrics\n",
    "    row = enriched_df.loc[idx]\n",
    "    # Get the OpenAIRE ID\n",
    "    identifier = row.get(\"OpenAIRE_ORG_ID\")\n",
    "    # define flag to indicate if ID was added\n",
    "    added_id = False\n",
    "    # Try to resolve OpenAIRE ID via ROR if not already present\n",
    "    if not identifier:\n",
    "        identifier = fetch_openorg_id_for_ror(row.get(\"ROR_LINK\") or row.get(\"ROR\"))\n",
    "        if identifier:\n",
    "            added_id = True\n",
    "    \n",
    "    # Collect metrics for the organization\n",
    "    metrics = collect_metrics(\"organization\", identifier)\n",
    "    result = {\n",
    "        \"OpenAIRE_ORG_ID\": identifier,\n",
    "    }\n",
    "\n",
    "    # Populate metric columns\n",
    "    if identifier and metrics:\n",
    "        # Calculate total research products\n",
    "        total_products = sum((metrics.get(label) or 0) for label in metric_columns[2:])\n",
    "        \n",
    "        for column in metric_columns:\n",
    "            if column == \"Total Research Products\":\n",
    "                result[column] = total_products\n",
    "            else:\n",
    "                result[column] = metrics.get(column)\n",
    "    else:\n",
    "        for column in metric_columns:\n",
    "            result[column] = pd.NA\n",
    "\n",
    "    result[\"added_id\"] = added_id\n",
    "    return idx, result\n",
    "\n",
    "# Enrich the DataFrame with OpenAIRE IDs and metrics using multithreading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# define progress bar\n",
    "progress = tqdm(enriched_df.index, desc=\"Enriching OpenAIRE IDs & metrics\", unit=\"org\")\n",
    "enriched_count = 0\n",
    "pending = set()\n",
    "\n",
    "# Use ThreadPoolExecutor to process rows concurrently\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = {executor.submit(enrich_row, idx): idx for idx in enriched_df.index}\n",
    "    for processed, future in enumerate(as_completed(futures), start=1):\n",
    "        idx, result = future.result()\n",
    "        enriched_df.at[idx, \"OpenAIRE_ORG_ID\"] = result[\"OpenAIRE_ORG_ID\"]\n",
    "        for column in metric_columns:\n",
    "            enriched_df.at[idx, column] = result[column]\n",
    "        if result.get(\"added_id\"):\n",
    "            enriched_count += 1\n",
    "        progress.update(1)\n",
    "        if processed % save_every == 0:\n",
    "            enriched_df.to_excel(checkpoint_path, index=False)\n",
    "            progress.set_postfix(saved_rows=processed)\n",
    "\n",
    "# Close the progress bar\n",
    "progress.close()\n",
    "\n",
    "# Save the final enriched DataFrame to Excel\n",
    "enriched_df.to_excel(output_path, index=False)\n",
    "\n",
    "# Print summary of enrichment process\n",
    "print(f\"Added OpenAIRE IDs for {enriched_count} organizations\")\n",
    "print(f\"Saved enriched data to {output_path}\")\n",
    "if checkpoint_path.exists():\n",
    "    print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "# Display the first few rows of the enriched DataFrame\n",
    "enriched_df.head()\n",
    "\n",
    "# Print summary of organizations still missing OpenAIRE IDs\n",
    "missing_ids = enriched_df[enriched_df[\"OpenAIRE_ORG_ID\"].isna() | (enriched_df[\"OpenAIRE_ORG_ID\"] == \"\")]\n",
    "if not missing_ids.empty:\n",
    "    print(\"Organizations still missing OpenAIRE IDs:\")\n",
    "    for name in missing_ids['name']:\n",
    "        print(f\"- {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75c17df",
   "metadata": {},
   "source": [
    "## 6. Visualise total research products per organisation\n",
    "Convert the enriched metrics to numeric form and plot the total research products per organisation (sorted descending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b4487",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [\n",
    "    \"Total Research Products\",\n",
    "    \"Publications\",\n",
    "    \"Research data\",\n",
    "    \"Research software\",\n",
    "    \"Other research products\",\n",
    "]\n",
    "\n",
    "for column in numeric_columns:\n",
    "    enriched_df[column] = pd.to_numeric(enriched_df[column], errors=\"coerce\")\n",
    "\n",
    "plot_df = enriched_df.sort_values(\"Total Research Products\", ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 20))\n",
    "has_missing_id = plot_df[\"OpenAIRE_ORG_ID\"].isna()\n",
    "bars = ax.barh(range(len(plot_df)), plot_df[\"Total Research Products\"])\n",
    "for idx, missing in enumerate(has_missing_id):\n",
    "    bars[idx].set_color(\"#cccccc\" if missing else \"#1f77b4\")\n",
    "\n",
    "ax.set_yticks(range(len(plot_df)))\n",
    "ax.set_yticklabels(plot_df[\"name\"])\n",
    "for idx, label in enumerate(ax.get_yticklabels()):\n",
    "    label.set_color(\"#cccccc\" if has_missing_id.iloc[idx] else \"black\")\n",
    "\n",
    "ax.set_xlabel(\"Total research products\")\n",
    "ax.set_ylabel(\"Organisation\")\n",
    "ax.set_title(\"Total research products per organisation\")\n",
    "fig.tight_layout()\n",
    "org_chart_path = IMG_DIR / \"org_total_products.png\"\n",
    "fig.savefig(org_chart_path, dpi=200, bbox_inches=\"tight\")\n",
    "print(f\"Saved organisation totals chart to {org_chart_path}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035456d5",
   "metadata": {},
   "source": [
    "## 7. Fetch data source metadata\n",
    "Retrieve every OpenAIRE data source linked to the enriched organisations and cache the registry details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c9a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Fetch and store OpenAIRE data source metadata for NL organizations\n",
    "\n",
    "# Define the columns for the data source metadata\n",
    "datasource_columns = [\n",
    "    \"OpenAIRE_ORG_ID\",\n",
    "    \"OpenAIRE_DataSource_ID\",\n",
    "    \"Name\",\n",
    "    \"Type\",\n",
    "    \"websiteUrl\",\n",
    "    \"OAI-endpoint\",\n",
    "    \"supports_NL-DIDL\",\n",
    "    \"support_OAI-DC\",\n",
    "    \"support_OAI-openaire\",\n",
    "    \"supports_RIOXX\",\n",
    "    \"support_OpenAIRE-CERIF\",\n",
    "    \"openaireCompatibility\",\n",
    "    \"Last_Indexed_Date\",\n",
    "    \"dateOfValidation\",\n",
    "]\n",
    "\n",
    "# Parse a single data source record from the API response\n",
    "def _parse_datasource_record(org_id: Optional[str], item: Dict[str, Any]) -> dict[str, Any]:\n",
    "    name = item.get(\"officialName\") or item.get(\"englishName\")\n",
    "    ds_type = (item.get(\"type\") or {}).get(\"value\")\n",
    "    record = {\n",
    "        \"OpenAIRE_ORG_ID\": org_id,\n",
    "        \"OpenAIRE_DataSource_ID\": item.get(\"id\"),\n",
    "        \"Name\": name,\n",
    "        \"Type\": ds_type,\n",
    "        \"websiteUrl\": item.get(\"websiteUrl\"),\n",
    "        \"OAI-endpoint\": None,\n",
    "        \"supports_NL-DIDL\": None,\n",
    "        \"support_OAI-DC\": None,\n",
    "        \"support_OAI-openaire\": None,\n",
    "        \"supports_RIOXX\": None,\n",
    "        \"support_OpenAIRE-CERIF\": None,\n",
    "        \"openaireCompatibility\": item.get(\"openaireCompatibility\"),\n",
    "        \"Last_Indexed_Date\": item.get(\"lastIndexedDate\") or item.get(\"lastIndexDate\"),\n",
    "        \"dateOfValidation\": item.get(\"dateOfValidation\"),\n",
    "    }\n",
    "    return record\n",
    "\n",
    "# Fetch all data sources for a given organization\n",
    "def fetch_datasources_for_org(org_entry: dict[str, Any]) -> list[dict[str, Any]]:\n",
    "    org_id = org_entry.get(\"OpenAIRE_ORG_ID\")\n",
    "    if not org_id:\n",
    "        return []\n",
    "\n",
    "    records: list[dict[str, Any]] = []\n",
    "    page = 1\n",
    "    page_size = 100\n",
    "    while True:\n",
    "        payload = call_graph_api(\n",
    "            \"/v1/dataSources\",\n",
    "            {\n",
    "                \"relOrganizationId\": org_id,\n",
    "                \"page\": page,\n",
    "                \"pageSize\": page_size,\n",
    "            },\n",
    "        )\n",
    "        results = payload.get(\"results\") or []\n",
    "        for item in results:\n",
    "            records.append(_parse_datasource_record(org_id, item))\n",
    "\n",
    "        header = payload.get(\"header\") or {}\n",
    "        num_found = header.get(\"numFound\", 0)\n",
    "        if page * page_size >= num_found or not results:\n",
    "            break\n",
    "        page += 1\n",
    "\n",
    "    return records\n",
    "\n",
    "# Fetch data sources for all organizations and compile into a DataFrame\n",
    "datasource_records: list[dict[str, Any]] = []\n",
    "org_entries = enriched_df.to_dict(\"records\")\n",
    "\n",
    "# Use ThreadPoolExecutor to fetch data sources concurrently\n",
    "with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "    futures = {executor.submit(fetch_datasources_for_org, entry): entry for entry in org_entries}\n",
    "    for future in tqdm(\n",
    "        as_completed(futures),\n",
    "        total=len(org_entries),\n",
    "        desc=\"Fetching data sources\",\n",
    "        unit=\"org\",\n",
    "    ):\n",
    "        datasource_records.extend(future.result())\n",
    "\n",
    "# Compile the data source records into a DataFrame\n",
    "datasources_df = pd.DataFrame(datasource_records, columns=datasource_columns)\n",
    "\n",
    "# Save the data sources DataFrame to Excel\n",
    "datasources_path = DATA_DIR / \"nl_orgs_openaire_datasources.xlsx\"\n",
    "datasources_df.to_excel(datasources_path, index=False)\n",
    "print(f\"Saved {len(datasources_df)} data source rows to {datasources_path}\")\n",
    "datasources_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3254bb",
   "metadata": {},
   "source": [
    "## 8. Capture data source content volumes\n",
    "Collect fresh numFound counts per data source (total and by product type) and store the snapshot with today's date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ad59bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if datasources_df.empty:\n",
    "    print(\"No data sources available; skipping numFound snapshot.\")\n",
    "    datasource_metrics_df = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"OpenAIRE_DataSource_ID\",\n",
    "            \"Name\",\n",
    "            \"Total Research Products\",\n",
    "            *PRODUCT_TYPE_LABELS.values(),\n",
    "            \"date_retrieved\",\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    snapshot_date = datetime.utcnow().date().isoformat()\n",
    "\n",
    "    def collect_datasource_counts(row: pd.Series) -> dict[str, Any]:\n",
    "        datasource_id = getattr(row, \"OpenAIRE_DataSource_ID\", None)\n",
    "        if not datasource_id:\n",
    "            return {}\n",
    "        metrics: dict[str, Any] = {\n",
    "            \"OpenAIRE_DataSource_ID\": datasource_id,\n",
    "            \"Name\": getattr(row, \"Name\", None),\n",
    "            \"date_retrieved\": snapshot_date,\n",
    "        }\n",
    "        total_params = {\"relCollectedFromDatasourceId\": datasource_id}\n",
    "        metrics[\"Total Research Products\"] = fetch_num_found(\n",
    "            \"/v2/researchProducts\", total_params\n",
    "        )\n",
    "        for rp_type, label in PRODUCT_TYPE_LABELS.items():\n",
    "            rp_params = {\"relCollectedFromDatasourceId\": datasource_id, \"type\": rp_type}\n",
    "            metrics[label] = fetch_num_found(\"/v2/researchProducts\", rp_params)\n",
    "        return metrics\n",
    "\n",
    "    datasource_metrics: list[dict[str, Any]] = []\n",
    "    with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "        futures = {\n",
    "            executor.submit(collect_datasource_counts, row): row\n",
    "            for row in datasources_df.itertuples(index=False)\n",
    "        }\n",
    "        for future in tqdm(\n",
    "            as_completed(futures),\n",
    "            total=len(futures),\n",
    "            desc=\"Collecting numFound\",\n",
    "            unit=\"datasource\",\n",
    "        ):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                datasource_metrics.append(result)\n",
    "\n",
    "    datasource_metrics_df = pd.DataFrame(\n",
    "        datasource_metrics,\n",
    "        columns=[\n",
    "            \"OpenAIRE_DataSource_ID\",\n",
    "            \"Name\",\n",
    "            \"Total Research Products\",\n",
    "            *PRODUCT_TYPE_LABELS.values(),\n",
    "            \"date_retrieved\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    snapshot_path = DATA_DIR / f\"nl_orgs_openaire_datasources_numFound_{snapshot_date}.xlsx\"\n",
    "    datasource_metrics_df.to_excel(snapshot_path, index=False)\n",
    "    print(\n",
    "        f\"Saved snapshot with {len(datasource_metrics_df)} data sources to {snapshot_path}\"\n",
    "    )\n",
    "\n",
    "datasource_metrics_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be14dd",
   "metadata": {},
   "source": [
    "## 9. Append snapshot to parquet history\n",
    "Keep a cumulative parquet log so repeated snapshots form a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699e34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = DATA_DIR / \"nl_orgs_openaire_datasources_numFound_history.xlsx\"\n",
    "if datasource_metrics_df.empty:\n",
    "    print(\"No snapshot data to append.\")\n",
    "else:\n",
    "    if history_path.exists():\n",
    "        historical_df = pd.read_excel(history_path)\n",
    "        combined_df = pd.concat([historical_df, datasource_metrics_df], ignore_index=True)\n",
    "    else:\n",
    "        combined_df = datasource_metrics_df.copy()\n",
    "\n",
    "    for column in [\n",
    "        \"Total Research Products\",\n",
    "        *PRODUCT_TYPE_LABELS.values(),\n",
    "    ]:\n",
    "        if column in combined_df.columns:\n",
    "            combined_df[column] = pd.to_numeric(combined_df[column], errors=\"coerce\")\n",
    "\n",
    "    combined_df.to_excel(history_path, index=False)\n",
    "    print(\n",
    "        f\"History now contains {len(combined_df)} rows at {history_path}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b33635f",
   "metadata": {},
   "source": [
    "## 10. Visualise data source totals\n",
    "Bar chart of the latest total research products per data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e5528",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = DATA_DIR / \"nl_orgs_openaire_datasources_numFound_history.xlsx\"\n",
    "datasources_path = DATA_DIR / \"nl_orgs_openaire_datasources.xlsx\"\n",
    "if not history_path.exists():\n",
    "    print(f\"History file {history_path} not found.\")\n",
    "else:\n",
    "    history_df = pd.read_excel(history_path)\n",
    "    numeric_columns = [\n",
    "        \"Total Research Products\",\n",
    "        *PRODUCT_TYPE_LABELS.values(),\n",
    "    ]\n",
    "    for column in numeric_columns:\n",
    "        if column in history_df.columns:\n",
    "            history_df[column] = pd.to_numeric(history_df[column], errors=\"coerce\")\n",
    "\n",
    "    history_df[\"date_retrieved\"] = pd.to_datetime(history_df[\"date_retrieved\"], errors=\"coerce\")\n",
    "    latest_date = history_df[\"date_retrieved\"].max()\n",
    "    latest_df = history_df[history_df[\"date_retrieved\"] == latest_date].copy()\n",
    "\n",
    "    if latest_df.empty:\n",
    "        print(\"No rows for the latest snapshot date.\")\n",
    "    else:\n",
    "        if \"Type\" not in latest_df.columns or latest_df[\"Type\"].isna().all():\n",
    "            if 'datasources_df' in globals():\n",
    "                type_lookup = datasources_df[[\"OpenAIRE_DataSource_ID\", \"Type\"]].copy()\n",
    "            elif datasources_path.exists():\n",
    "                type_lookup = pd.read_excel(datasources_path)[\n",
    "                    [\"OpenAIRE_DataSource_ID\", \"Type\"]\n",
    "                ].copy()\n",
    "            else:\n",
    "                type_lookup = pd.DataFrame(columns=[\"OpenAIRE_DataSource_ID\", \"Type\"])\n",
    "\n",
    "            latest_df = latest_df.merge(\n",
    "                type_lookup,\n",
    "                on=\"OpenAIRE_DataSource_ID\",\n",
    "                how=\"left\",\n",
    "                suffixes=(None, \"_ds\"),\n",
    "            )\n",
    "            if \"Type_ds\" in latest_df.columns:\n",
    "                latest_df[\"Type\"] = latest_df[\"Type\"].fillna(latest_df[\"Type_ds\"])\n",
    "                latest_df = latest_df.drop(columns=[\"Type_ds\"])\n",
    "\n",
    "        zero_mask = latest_df[\"Total Research Products\"].fillna(0) <= 0\n",
    "        zero_total_df = latest_df[zero_mask].copy()\n",
    "        if not zero_total_df.empty:\n",
    "            print(\"Datasources with zero total research products (excluded from chart):\")\n",
    "            for _, row in zero_total_df.iterrows():\n",
    "                print(f\" - {row.get('Name', 'Unknown')} ({row.get('OpenAIRE_DataSource_ID')})\")\n",
    "\n",
    "        latest_df = latest_df.loc[~zero_mask].copy()\n",
    "        if latest_df.empty:\n",
    "            print(\"All datasources reported zero totals; skipping chart.\")\n",
    "        else:\n",
    "            latest_df = latest_df.sort_values(\"Total Research Products\", ascending=False)\n",
    "            max_label_length = 40\n",
    "            display_names = (\n",
    "                latest_df[\"Name\"].fillna(\"Unknown\")\n",
    "                .astype(str)\n",
    "                .apply(lambda text: text if len(text) <= max_label_length else text[: max_label_length - 1] + \"…\")\n",
    "            )\n",
    "\n",
    "            type_series = latest_df[\"Type\"].fillna(\"Unknown\")\n",
    "            unique_types = type_series.unique()\n",
    "            cmap = plt.cm.get_cmap(\"tab20\", max(len(unique_types), 1))\n",
    "            color_map = {t: cmap(i) for i, t in enumerate(unique_types)}\n",
    "            colors = type_series.map(color_map)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(12, 10))\n",
    "            ax.barh(display_names, latest_df[\"Total Research Products\"], color=colors)\n",
    "            ax.set_title(\n",
    "                f\"Total research products per data source (snapshot {latest_date.date()})\"\n",
    "            )\n",
    "            ax.set_xlabel(\"Total research products\")\n",
    "            ax.set_ylabel(\"Data source\")\n",
    "            ax.invert_yaxis()\n",
    "\n",
    "            handles = [plt.Rectangle((0, 0), 1, 1, color=color_map[t]) for t in unique_types]\n",
    "            ax.legend(\n",
    "                handles,\n",
    "                unique_types,\n",
    "                title=\"Data source type\",\n",
    "                loc=\"upper center\",\n",
    "                bbox_to_anchor=(0.5, -0.12),\n",
    "                ncol=3,\n",
    "            )\n",
    "            fig.tight_layout()\n",
    "            chart_path = IMG_DIR / \"datasource_totals_latest.png\"\n",
    "            fig.savefig(chart_path, dpi=200, bbox_inches=\"tight\")\n",
    "            print(f\"Saved latest data source totals chart to {chart_path}\")\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5204e1ff",
   "metadata": {},
   "source": [
    "## 11. Compare latest vs previous snapshot\n",
    "Show side-by-side totals for the latest two snapshots per data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76950aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = DATA_DIR / \"nl_orgs_openaire_datasources_numFound_history.xlsx\"\n",
    "datasources_path = DATA_DIR / \"nl_orgs_openaire_datasources.xlsx\"\n",
    "if not history_path.exists():\n",
    "    print(f\"History file {history_path} not found.\")\n",
    "else:\n",
    "    history_df = pd.read_excel(history_path)\n",
    "    numeric_columns = [\n",
    "        \"Total Research Products\",\n",
    "        *PRODUCT_TYPE_LABELS.values(),\n",
    "    ]\n",
    "    for column in numeric_columns:\n",
    "        if column in history_df.columns:\n",
    "            history_df[column] = pd.to_numeric(history_df[column], errors=\"coerce\")\n",
    "\n",
    "    history_df[\"date_retrieved\"] = pd.to_datetime(history_df[\"date_retrieved\"], errors=\"coerce\")\n",
    "    unique_dates = sorted(history_df[\"date_retrieved\"].dropna().unique())\n",
    "    if len(unique_dates) < 2:\n",
    "        print(\"Not enough snapshots to compare (need at least two dates).\")\n",
    "    else:\n",
    "        latest_date = unique_dates[-1]\n",
    "        previous_date = unique_dates[-2]\n",
    "        latest_df = history_df[history_df[\"date_retrieved\"] == latest_date].copy()\n",
    "        previous_df = history_df[history_df[\"date_retrieved\"] == previous_date].copy()\n",
    "\n",
    "        if 'datasources_df' in globals():\n",
    "            type_lookup = datasources_df[[\"OpenAIRE_DataSource_ID\", \"Type\"]].copy()\n",
    "        elif datasources_path.exists():\n",
    "            type_lookup = pd.read_excel(datasources_path)[\n",
    "                [\"OpenAIRE_DataSource_ID\", \"Type\"]\n",
    "            ].copy()\n",
    "        else:\n",
    "            type_lookup = pd.DataFrame(columns=[\"OpenAIRE_DataSource_ID\", \"Type\"])\n",
    "\n",
    "        def attach_type(df: pd.DataFrame) -> pd.DataFrame:\n",
    "            if \"Type\" in df.columns and df[\"Type\"].notna().any():\n",
    "                return df\n",
    "            merged = df.merge(\n",
    "                type_lookup,\n",
    "                on=\"OpenAIRE_DataSource_ID\",\n",
    "                how=\"left\",\n",
    "                suffixes=(None, \"_ds\"),\n",
    "            )\n",
    "            if \"Type_ds\" in merged.columns:\n",
    "                merged[\"Type\"] = merged[\"Type\"].fillna(merged[\"Type_ds\"])\n",
    "                merged = merged.drop(columns=[\"Type_ds\"])\n",
    "            return merged\n",
    "\n",
    "        latest_df = attach_type(latest_df)\n",
    "        previous_df = attach_type(previous_df)\n",
    "\n",
    "        combined = latest_df[[\n",
    "            \"OpenAIRE_DataSource_ID\",\n",
    "            \"Name\",\n",
    "            \"Type\",\n",
    "            \"Total Research Products\",\n",
    "        ]].rename(columns={\"Total Research Products\": \"Total Research Products_latest\"})\n",
    "\n",
    "        combined = combined.merge(\n",
    "            previous_df[[\"OpenAIRE_DataSource_ID\", \"Total Research Products\"]]\n",
    "            .rename(columns={\"Total Research Products\": \"Total Research Products_previous\"}),\n",
    "            on=\"OpenAIRE_DataSource_ID\",\n",
    "            how=\"outer\",\n",
    "        )\n",
    "\n",
    "        combined[\"Name\"] = combined[\"Name\"].fillna(\"Unknown\")\n",
    "        combined[\"Type\"] = combined[\"Type\"].fillna(\"Unknown\")\n",
    "        combined = combined.fillna({\n",
    "            \"Total Research Products_latest\": 0,\n",
    "            \"Total Research Products_previous\": 0,\n",
    "        })\n",
    "\n",
    "        zero_mask = combined[\"Total Research Products_latest\"].fillna(0) <= 0\n",
    "        zero_total_df = combined[zero_mask].copy()\n",
    "        if not zero_total_df.empty:\n",
    "            print(\"Datasources with zero total research products (excluded from comparison chart):\")\n",
    "            for _, row in zero_total_df.iterrows():\n",
    "                print(f\" - {row.get('Name', 'Unknown')} ({row.get('OpenAIRE_DataSource_ID')})\")\n",
    "\n",
    "        combined = combined.loc[~zero_mask].copy()\n",
    "        if combined.empty:\n",
    "            print(\"No datasources with non-zero totals available for comparison chart.\")\n",
    "        else:\n",
    "            combined = combined.sort_values(\"Total Research Products_latest\", ascending=False)\n",
    "            max_label_length = 40\n",
    "            display_names = (\n",
    "                combined[\"Name\"].astype(str)\n",
    "                .apply(lambda text: text if len(text) <= max_label_length else text[: max_label_length - 1] + \"…\")\n",
    "            )\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(12, 30))\n",
    "            y_positions = range(len(combined))\n",
    "            bar_height = 0.35\n",
    "            ax.barh(\n",
    "                [y + bar_height / 2 for y in y_positions],\n",
    "                combined[\"Total Research Products_latest\"],\n",
    "                height=bar_height,\n",
    "                color=\"#1a9850\",\n",
    "                label=f\"Latest ({latest_date.date()})\",\n",
    "            )\n",
    "            ax.barh(\n",
    "                [y - bar_height / 2 for y in y_positions],\n",
    "                combined[\"Total Research Products_previous\"],\n",
    "                height=bar_height,\n",
    "                color=\"#b8e186\",\n",
    "                label=f\"Previous ({previous_date.date()})\",\n",
    "            )\n",
    "\n",
    "            ax.set_yticks(list(y_positions))\n",
    "            ax.set_yticklabels(display_names)\n",
    "            ax.set_xlabel(\"Total research products\")\n",
    "            ax.set_title(\"Latest vs previous total research products per data source\")\n",
    "            ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.08), ncol=2)\n",
    "            ax.invert_yaxis()\n",
    "            fig.tight_layout()\n",
    "            compare_path = IMG_DIR / \"datasource_totals_compare.png\"\n",
    "            fig.savefig(compare_path, dpi=200, bbox_inches=\"tight\")\n",
    "            print(f\"Saved comparison chart to {compare_path}\")\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7e5f7a",
   "metadata": {},
   "source": [
    "## 12. Organisation vs. datasource totals comparison\n",
    "Compare each organisation's total research products with the combined totals of all datasources linked to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9779988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = DATA_DIR / \"nl_orgs_openaire_datasources_numFound_history.xlsx\"\n",
    "datasources_path = DATA_DIR / \"nl_orgs_openaire_datasources.xlsx\"\n",
    "orgs_path = DATA_DIR / \"nl_orgs_openaire.xlsx\"\n",
    "if not (history_path.exists() and datasources_path.exists() and orgs_path.exists()):\n",
    "    print(\"One or more required files are missing. Please run Steps 3, 7, and 8 before this comparison.\")\n",
    "else:\n",
    "    history_df = pd.read_excel(history_path)\n",
    "    datasources_df = pd.read_excel(datasources_path)\n",
    "    orgs_df = pd.read_excel(orgs_path)\n",
    "\n",
    "    numeric_columns = [\n",
    "        \"Total Research Products\",\n",
    "        *PRODUCT_TYPE_LABELS.values(),\n",
    "    ]\n",
    "    for column in numeric_columns:\n",
    "        if column in history_df.columns:\n",
    "            history_df[column] = pd.to_numeric(history_df[column], errors=\"coerce\")\n",
    "        if column in orgs_df.columns:\n",
    "            orgs_df[column] = pd.to_numeric(orgs_df[column], errors=\"coerce\")\n",
    "\n",
    "    history_df[\"date_retrieved\"] = pd.to_datetime(history_df[\"date_retrieved\"], errors=\"coerce\")\n",
    "    latest_date = history_df[\"date_retrieved\"].max()\n",
    "    datasource_latest = history_df[history_df[\"date_retrieved\"] == latest_date].copy()\n",
    "\n",
    "    if datasource_latest.empty:\n",
    "        print(\"No datasource snapshot found for comparison.\")\n",
    "    else:\n",
    "        datasource_latest[\"Total Research Products\"] = datasource_latest[\"Total Research Products\"].fillna(0)\n",
    "        link_df = datasources_df[[\"OpenAIRE_DataSource_ID\", \"OpenAIRE_ORG_ID\"]].copy()\n",
    "        mapped = link_df.merge(\n",
    "            datasource_latest[[\"OpenAIRE_DataSource_ID\", \"Total Research Products\"]],\n",
    "            on=\"OpenAIRE_DataSource_ID\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        mapped[\"Total Research Products\"] = mapped[\"Total Research Products\"].fillna(0)\n",
    "        datasource_by_org = mapped.groupby(\"OpenAIRE_ORG_ID\")[\"Total Research Products\"].sum()\n",
    "\n",
    "        org_totals = orgs_df[[\"name\", \"OpenAIRE_ORG_ID\", \"Total Research Products\"]].copy()\n",
    "        org_totals[\"Total Research Products\"] = org_totals[\"Total Research Products\"].fillna(0)\n",
    "        org_totals[\"Datasource totals\"] = org_totals[\"OpenAIRE_ORG_ID\"].map(datasource_by_org).fillna(0)\n",
    "\n",
    "        if org_totals.empty:\n",
    "            print(\"Organisation totals not available for comparison.\")\n",
    "        else:\n",
    "            org_totals = org_totals.sort_values(\"Total Research Products\", ascending=False)\n",
    "            max_label_length = 40\n",
    "            display_names = org_totals[\"name\"].astype(str).apply(\n",
    "                lambda text: text if len(text) <= max_label_length else text[: max_label_length - 1] + \"…\"\n",
    "            )\n",
    "            indices = range(len(org_totals))\n",
    "            bar_height = 0.4\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(12, 20))\n",
    "            ax.barh(\n",
    "                [i + bar_height / 2 for i in indices],\n",
    "                org_totals[\"Total Research Products\"],\n",
    "                height=bar_height,\n",
    "                color=\"#1f77b4\",\n",
    "                label=\"Organisation total\",\n",
    "            )\n",
    "            ax.barh(\n",
    "                [i - bar_height / 2 for i in indices],\n",
    "                org_totals[\"Datasource totals\"],\n",
    "                height=bar_height,\n",
    "                color=\"#2ca02c\",\n",
    "                label=\"Combined datasource totals\",\n",
    "            )\n",
    "\n",
    "            ax.set_yticks(list(indices))\n",
    "            ax.set_yticklabels(display_names)\n",
    "            ax.set_xlabel(\"Total research products\")\n",
    "            ax.set_ylabel(\"Organisation\")\n",
    "            ax.set_title(\n",
    "                f\"Organisation vs. datasource totals (snapshot {latest_date.date()})\"\n",
    "            )\n",
    "            ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.08), ncol=2)\n",
    "            ax.invert_yaxis()\n",
    "            fig.tight_layout()\n",
    "            org_vs_ds_path = IMG_DIR / \"org_vs_datasources.png\"\n",
    "            fig.savefig(org_vs_ds_path, dpi=200, bbox_inches=\"tight\")\n",
    "            print(f\"Saved organisation vs datasource comparison chart to {org_vs_ds_path}\")\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa59974",
   "metadata": {},
   "source": [
    "## 13. Organisation vs. individual datasource breakdown\n",
    "Display each organisation alongside its own datasources for the latest snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb407d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = DATA_DIR / \"nl_orgs_openaire_datasources_numFound_history.xlsx\"\n",
    "datasources_path = DATA_DIR / \"nl_orgs_openaire_datasources.xlsx\"\n",
    "orgs_path = DATA_DIR / \"nl_orgs_openaire.xlsx\"\n",
    "if not (history_path.exists() and datasources_path.exists() and orgs_path.exists()):\n",
    "    print(\"One or more required files are missing. Run Steps 3, 7, and 8 first.\")\n",
    "else:\n",
    "    history_df = pd.read_excel(history_path)\n",
    "    datasources_df = pd.read_excel(datasources_path)\n",
    "    orgs_df = pd.read_excel(orgs_path)\n",
    "\n",
    "    numeric_columns = [\n",
    "        \"Total Research Products\",\n",
    "        *PRODUCT_TYPE_LABELS.values(),\n",
    "    ]\n",
    "    for column in numeric_columns:\n",
    "        if column in history_df.columns:\n",
    "            history_df[column] = pd.to_numeric(history_df[column], errors=\"coerce\")\n",
    "        if column in orgs_df.columns:\n",
    "            orgs_df[column] = pd.to_numeric(orgs_df[column], errors=\"coerce\")\n",
    "\n",
    "    history_df[\"date_retrieved\"] = pd.to_datetime(history_df[\"date_retrieved\"], errors=\"coerce\")\n",
    "    latest_date = history_df[\"date_retrieved\"].max()\n",
    "    datasource_latest = history_df[history_df[\"date_retrieved\"] == latest_date].copy()\n",
    "\n",
    "    if datasource_latest.empty:\n",
    "        print(\"No datasource snapshot found for breakdown chart.\")\n",
    "    else:\n",
    "        datasource_latest[\"Total Research Products\"] = datasource_latest[\"Total Research Products\"].fillna(0)\n",
    "        ds_with_org = datasources_df[[\n",
    "            \"OpenAIRE_DataSource_ID\",\n",
    "            \"OpenAIRE_ORG_ID\",\n",
    "            \"Name\",\n",
    "        ]].merge(\n",
    "            datasource_latest[[\"OpenAIRE_DataSource_ID\", \"Total Research Products\"]],\n",
    "            on=\"OpenAIRE_DataSource_ID\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        ds_with_org[\"Total Research Products\"] = ds_with_org[\"Total Research Products\"].fillna(0)\n",
    "        orgs_df[\"Total Research Products\"] = orgs_df[\"Total Research Products\"].fillna(0)\n",
    "\n",
    "        records = []\n",
    "        zero_datasources: list[str] = []\n",
    "\n",
    "        for _, org_row in orgs_df.sort_values(\"Total Research Products\", ascending=False).iterrows():\n",
    "            org_name = org_row.get(\"name\", \"Unknown organisation\")\n",
    "            org_id = org_row.get(\"OpenAIRE_ORG_ID\")\n",
    "            org_value = float(org_row.get(\"Total Research Products\") or 0)\n",
    "            records.append({\"label\": org_name, \"value\": org_value, \"color\": \"#1f77b4\"})\n",
    "\n",
    "            org_ds = ds_with_org[ds_with_org[\"OpenAIRE_ORG_ID\"] == org_id]\n",
    "            if org_ds.empty:\n",
    "                continue\n",
    "\n",
    "            for _, ds_row in org_ds.sort_values(\"Total Research Products\", ascending=False).iterrows():\n",
    "                ds_value = float(ds_row.get(\"Total Research Products\") or 0)\n",
    "                ds_name = ds_row.get(\"Name\") or ds_row.get(\"OpenAIRE_DataSource_ID\")\n",
    "                if ds_value <= 0:\n",
    "                    zero_datasources.append(f\"{ds_name} (org: {org_name})\")\n",
    "                    continue\n",
    "                label = f\"  ↳ {ds_name}\"\n",
    "                records.append({\"label\": label, \"value\": ds_value, \"color\": \"#2ca02c\"})\n",
    "\n",
    "        if zero_datasources:\n",
    "            print(\"Datasources with zero totals (excluded):\")\n",
    "            for entry in zero_datasources:\n",
    "                print(f\" - {entry}\")\n",
    "\n",
    "        if not records:\n",
    "            print(\"No datapoints available for the breakdown chart.\")\n",
    "    \n",
    "        else:\n",
    "            values = [rec[\"value\"] for rec in records]\n",
    "            labels = [rec[\"label\"] for rec in records]\n",
    "            colors = [rec[\"color\"] for rec in records]\n",
    "            fig_height = max(6, 0.4 * len(records))\n",
    "            fig, ax = plt.subplots(figsize=(14, fig_height))\n",
    "            ax.barh(range(len(records)), values, color=colors)\n",
    "            ax.set_yticks(range(len(records)))\n",
    "            ax.set_yticklabels(labels)\n",
    "            ax.set_xlabel(\"Total research products\")\n",
    "            ax.set_ylabel(\"Organisation / Datasource\")\n",
    "            ax.set_title(\n",
    "                f\"Organisation vs. individual datasource totals (snapshot {latest_date.date()})\"\n",
    "            )\n",
    "            ax.invert_yaxis()\n",
    "            fig.tight_layout()\n",
    "            breakdown_path = IMG_DIR / \"org_vs_datasource_breakdown.png\"\n",
    "            fig.savefig(breakdown_path, dpi=200, bbox_inches=\"tight\")\n",
    "            print(f\"Saved organisation/datasource breakdown chart to {breakdown_path}\")\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f84dd7a",
   "metadata": {},
   "source": [
    "## 14. Align repository overview with datasource IDs\n",
    "Download the public repository workbook, align each row with the datasource map produced earlier, and store the enriched result in `/data`. This gives repository managers the exact `OpenAIRE_DataSource_ID` backing every entry across the CRIS/Literature/Data sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592128bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "REPOSITORIES_URL = (\n",
    "    \"https://docs.google.com/spreadsheets/d/e/\"\n",
    "    \"2PACX-1vQQfWwRstnv7t3sQW00zIIik6uyEW4W-4JvaU2M1nQXtwF9RRDy5P46Mz063NAIig/pub?output=xlsx\"\n",
    ")\n",
    "repositories_path = DATA_DIR / \"overzicht_repositories.xlsx\"\n",
    "datasources_path = DATA_DIR / \"nl_orgs_openaire_datasources.xlsx\"\n",
    "aligned_path = DATA_DIR / \"overzicht_repositories_with_datasource.xlsx\"\n",
    "\n",
    "if not datasources_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing datasource export: {datasources_path}. Run step 7 before this alignment.\"\n",
    "    )\n",
    "\n",
    "response = requests.get(REPOSITORIES_URL, timeout=30)\n",
    "response.raise_for_status()\n",
    "repositories_path.write_bytes(response.content)\n",
    "print(f\"Downloaded repository overview to {repositories_path}\")\n",
    "\n",
    "xls = pd.ExcelFile(repositories_path)\n",
    "ds = pd.read_excel(datasources_path)\n",
    "\n",
    "if \"OAI-endpoint\" not in ds.columns or \"OpenAIRE_DataSource_ID\" not in ds.columns:\n",
    "    raise ValueError(\"Datasource export missing required columns (OAI-endpoint/OpenAIRE_DataSource_ID).\")\n",
    "\n",
    "\n",
    "def extract_domain(url: str):\n",
    "    if isinstance(url, str) and url.strip():\n",
    "        candidate = url.strip()\n",
    "        if not candidate.startswith((\"http://\", \"https://\")):\n",
    "            candidate = \"http://\" + candidate\n",
    "        try:\n",
    "            return urlparse(candidate).netloc.lower()\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "MULTI_TLD = {\n",
    "    \"co.uk\",\"ac.uk\",\"gov.uk\",\"org.uk\",\"sch.uk\",\n",
    "    \"com.au\",\"net.au\",\"edu.au\",\"gov.au\",\"org.au\",\n",
    "    \"co.nz\",\"org.nz\",\"gov.nz\",\"ac.nz\",\n",
    "    \"co.jp\",\"ne.jp\",\"ac.jp\",\"or.jp\",\n",
    "}\n",
    "\n",
    "\n",
    "def extract_root_domain(domain: str):\n",
    "    if not isinstance(domain, str) or not domain:\n",
    "        return None\n",
    "    parts = domain.split(\".\")\n",
    "    if len(parts) <= 2:\n",
    "        return domain\n",
    "    last_two = \".\".join(parts[-2:])\n",
    "    last_three = \".\".join(parts[-3:])\n",
    "    if last_two in MULTI_TLD and len(parts) >= 3:\n",
    "        return \".\".join(parts[-3:])\n",
    "    if last_three in MULTI_TLD and len(parts) >= 4:\n",
    "        return \".\".join(parts[-4:])\n",
    "    return last_two\n",
    "\n",
    "\n",
    "ds[\"domain\"] = ds[\"OAI-endpoint\"].apply(extract_domain)\n",
    "ds[\"root_domain\"] = ds[\"domain\"].apply(extract_root_domain)\n",
    "\n",
    "type_filters = {\n",
    "    \"CRIS\": [\"CRIS\"],\n",
    "    \"Literature Repository\": [\"Institutional Repository\"],\n",
    "    \"Data Repository\": [\"Thematic Repository\", \"Data Repository\"],\n",
    "}\n",
    "\n",
    "candidate_cols = [\n",
    "    \"Baseurl in Narcis\",\n",
    "    \"Baseurl Narcis\",\n",
    "    \"Endpoint\",\n",
    "    \"OAI-endpoint\",\n",
    "    \"Endpoint OpenDOAR\",\n",
    "]\n",
    "\n",
    "with pd.ExcelWriter(aligned_path, engine=\"openpyxl\") as writer:\n",
    "    for sheet in xls.sheet_names:\n",
    "        df = pd.read_excel(repositories_path, sheet_name=sheet)\n",
    "        df[\"domain\"] = None\n",
    "        for col in candidate_cols:\n",
    "            if col in df.columns:\n",
    "                df[\"domain\"] = df[\"domain\"].fillna(df[col].apply(extract_domain))\n",
    "        df[\"root_domain\"] = df[\"domain\"].apply(extract_root_domain)\n",
    "\n",
    "        dsf = ds.copy()\n",
    "        if sheet in type_filters and \"Type\" in dsf.columns:\n",
    "            allowed = type_filters[sheet]\n",
    "            mask = pd.Series(False, index=dsf.index)\n",
    "            for keyword in allowed:\n",
    "                mask |= dsf[\"Type\"].str.contains(keyword, case=False, na=False)\n",
    "            dsf = dsf[mask].copy()\n",
    "\n",
    "        merged = df.merge(\n",
    "            dsf[[\"domain\", \"root_domain\", \"OpenAIRE_DataSource_ID\"]],\n",
    "            on=\"domain\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "\n",
    "        needs_fallback = merged[\"OpenAIRE_DataSource_ID\"].isna()\n",
    "        if needs_fallback.any():\n",
    "            rd_map = (\n",
    "                dsf.dropna(subset=[\"root_domain\"])\n",
    "                   .drop_duplicates(subset=[\"root_domain\"])\n",
    "                   .set_index(\"root_domain\")[\"OpenAIRE_DataSource_ID\"]\n",
    "                   .to_dict()\n",
    "            )\n",
    "            merged.loc[needs_fallback, \"OpenAIRE_DataSource_ID\"] = (\n",
    "                merged.loc[needs_fallback, \"root_domain\"].map(rd_map)\n",
    "            )\n",
    "\n",
    "        merged.to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "print(f\"Aligned repository workbook written to {aligned_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
