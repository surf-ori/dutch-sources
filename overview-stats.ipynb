{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "696654e4",
   "metadata": {},
   "source": [
    "# OpenAIRE Graph University Overview\n",
    "\n",
    "This notebook fetches summary statistics per university using the [OpenAIRE Graph API](https://graph.openaire.eu/docs/apis/graph-api/). For every listed university we compare three perspectives:\n",
    "\n",
    "- **A. Publications affiliated to the university** – filter on the OpenAIRE OpenORG identifier.\n",
    "- **B. Publications collected by the main (CRIS) data source** – filter on the CRS/data source identifier.\n",
    "- **C. Publications collected by the secondary repository** – filter on the repository identifier when available.\n",
    "\n",
    "For each perspective we retrieve counts for funding/projects, data sources, and research products split into publications, datasets, software, and other research outputs. The notebook separates the data collection steps so that each can be re-run independently when debugging or iterating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca1ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages (if not already installed)\n",
    "\n",
    "!pip install pandas matplotlib openpyxl python-dotenv requests tqdm pyarrow plotly ipywidgets \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86519c7f",
   "metadata": {},
   "source": [
    "## 1. Imports and reused constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3323f333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, Optional, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.float_format\", lambda value: f\"{value:,.0f}\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "CLIENT_ID = os.getenv(\"CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"CLIENT_SECRET\")\n",
    "\n",
    "if not CLIENT_ID or not CLIENT_SECRET:\n",
    "    raise RuntimeError(\n",
    "        \"Missing OpenAIRE credentials. Set CLIENT_ID and CLIENT_SECRET in the environment.\"\n",
    "    )\n",
    "\n",
    "BASE_URL = \"https://api.openaire.eu/graph\"\n",
    "TOKEN_URL = \"https://aai.openaire.eu/oidc/token\"\n",
    "API_USER_AGENT = \"OpenAIRE-tools overview-stats notebook\"\n",
    "API_PAUSE_SECONDS = 0.1  # throttle requests a bit to stay within rate limits\n",
    "TOKEN_REFRESH_BUFFER = 60  # refresh the token one minute before expiration\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "IMG_DIR = Path(\"img\")\n",
    "IMG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "PRODUCT_TYPE_LABELS = {\n",
    "    \"publication\": \"Publications\",\n",
    "    \"dataset\": \"Research data\",\n",
    "    \"software\": \"Research software\",\n",
    "    \"other\": \"Other research products\",\n",
    "}\n",
    "\n",
    "METRIC_ORDER = [\n",
    "    \"Funding / Projects\",\n",
    "    \"Data sources\",\n",
    "    *PRODUCT_TYPE_LABELS.values(),\n",
    "]\n",
    "\n",
    "COMPARISON_LONG_PATH = DATA_DIR / \"comparison_long.csv\"\n",
    "COMPARISON_PIVOT_PATH = DATA_DIR / \"comparison_pivot.csv\"\n",
    "\n",
    "_access_token: Optional[str] = None\n",
    "_access_token_expiry: float = 0.0\n",
    "\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e359a7",
   "metadata": {},
   "source": [
    "## 2. Parse the university reference table\n",
    "The raw table below mirrors the values supplied in the request. We reshape it into a structured list so that the rest of the notebook can iterate over the entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42836ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the NL organizations reference table\n",
    "# Download the latest NL organizations baseline from the Google Sheets URL\n",
    "\n",
    "# URL of the published Google Sheets NL organizations baseline\n",
    "nl_orgs_baseline_url = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vTDQiWDIaI1SZkPTMNCovicBhA-nQND1drXoUKvrG1O_Ga3hLDRvmQZao_TvNgmNQ/pub?output=xlsx\"\n",
    "# Local path to store the downloaded baseline\n",
    "NL_ORGS_BASELINE_PATH = DATA_DIR / \"nl_orgs_baseline.xlsx\"\n",
    "# Name of the sheet containing the NL organizations data\n",
    "NL_ORGS_SHEET_NAME = \"nl-orgs\"\n",
    "\n",
    "# Function to download the NL organizations baseline\n",
    "def download_nl_orgs_baseline(url: str, dest: Path) -> Path:\n",
    "    \"\"\"Download the latest NL organizations baseline and store it locally.\"\"\"\n",
    "    # Ensure the destination directory exists\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if dest.exists():\n",
    "        print(f\"Using existing baseline file: {dest}\")\n",
    "        return dest\n",
    "        \n",
    "    print(f\"Downloading baseline file from {url}\")\n",
    "    # Download the file\n",
    "    response = requests.get(url, timeout=120)\n",
    "    response.raise_for_status()\n",
    "    dest.write_bytes(response.content)\n",
    "    print(f\"Saved baseline file to: {dest}\")\n",
    "    return dest\n",
    "\n",
    "# Download the NL organizations baseline\n",
    "download_nl_orgs_baseline(nl_orgs_baseline_url, NL_ORGS_BASELINE_PATH)\n",
    "\n",
    "# Set the table path to the downloaded baseline\n",
    "TABLE_PATH = NL_ORGS_BASELINE_PATH\n",
    "\n",
    "# Load and parse the NL organizations reference table\n",
    "def load_university_table(path: Union[str, Path], sheet_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the NL organizations reference table and normalise empty values to blanks.\"\"\"\n",
    "    table_path = Path(path)\n",
    "    if not table_path.exists():\n",
    "        raise FileNotFoundError(f\"Reference table not found: {table_path}\")\n",
    "    excel_suffixes = {\".xlsx\", \".xls\"}\n",
    "    if table_path.suffix.lower() in excel_suffixes:\n",
    "        # Load the Excel file\n",
    "        df = pd.read_excel(table_path, sheet_name=sheet_name, dtype=str, keep_default_na=False)\n",
    "    else:\n",
    "        # Load as a TSV/CSV file\n",
    "        df = pd.read_csv(table_path, sep=\"\t\", dtype=str, keep_default_na=False)\n",
    "    return df.fillna(\"\")\n",
    "\n",
    "# Helper function to pick the first non-empty value from a list of columns\n",
    "def _pick(row: pd.Series, *columns: str) -> str:\n",
    "    for column in columns:\n",
    "        if column not in row:\n",
    "            continue\n",
    "        value = row[column]\n",
    "        if isinstance(value, str):\n",
    "            value = value.strip()\n",
    "        if value:\n",
    "            return value\n",
    "    return \"\"\n",
    "\n",
    "# Normalize ROR link\n",
    "def normalise_ror_link(value: str) -> str:\n",
    "    # Normalize ROR identifier to a full URL\n",
    "    value = (value or \"\").strip()\n",
    "    # If the value is empty after stripping, return an empty string\n",
    "    if not value:\n",
    "        return \"\"\n",
    "    # If the value already starts with \"http\", return it as is\n",
    "    if value.startswith(\"http\"):\n",
    "        return value\n",
    "    # If the value starts with \"ror.org\", prepend \"https://\"\n",
    "    if value.startswith(\"ror.org\"):\n",
    "        return f\"https://{value}\"\n",
    "    # Otherwise, assume it's a ROR ID and construct the full URL\n",
    "    value = value.strip().strip(\"/\")\n",
    "    # If the value is empty after stripping, return an empty string\n",
    "    if not value:\n",
    "        return \"\"\n",
    "    # Return the full ROR URL\n",
    "    return f\"https://ror.org/{value}\"\n",
    "\n",
    "# Extract ROR ID from a value\n",
    "def extract_ror_id(value: str) -> str:\n",
    "    value = (value or \"\").strip()\n",
    "    if not value:\n",
    "        return \"\"\n",
    "    if value.startswith(\"http\"):\n",
    "        value = value.rstrip(\"/\").split(\"/\")[-1]\n",
    "    return value\n",
    "\n",
    "# Parse university rows from the DataFrame\n",
    "def parse_university_rows(df: pd.DataFrame) -> list[dict[str, Optional[str]]]:\n",
    "    # Parse university rows from the DataFrame and extract relevant fields\n",
    "    parsed: list[dict[str, Optional[str]]] = []\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Determine the university name using multiple possible columns\n",
    "        name = _pick(row, \"full_name_in_English\", \"University\", \"organization_name\")\n",
    "        # Skip rows without a valid name\n",
    "        if not name:\n",
    "            continue\n",
    "        \n",
    "        # Extract and normalize ROR information\n",
    "        ror_id = extract_ror_id(_pick(row, \"ROR\"))\n",
    "        # Normalize ROR link\n",
    "        ror_link = normalise_ror_link(_pick(row, \"ROR_LINK\") or ror_id)\n",
    "        # If ROR link is available but ROR ID is not, extract ROR ID from the link\n",
    "        if ror_link and not ror_id:\n",
    "            # Extract ROR ID from the normalized ROR link\n",
    "            ror_id = extract_ror_id(ror_link)\n",
    "\n",
    "        # Construct the university entry\n",
    "        entry = {\n",
    "            \"name\": name,\n",
    "            \"acronym_EN\": _pick(row, \"acronym_EN\") or None,\n",
    "            \"acronym_AGG\": _pick(row, \"acronym_AGG\") or None,\n",
    "            \"grouping\": _pick(row, \"main_grouping\") or None,\n",
    "            \"ROR\": ror_id or None,\n",
    "            \"ROR_LINK\": ror_link or None,\n",
    "            \"OpenAIRE_ORG_ID\": _pick(row, \"OpenAIRE_ORG_ID\", \"OpenAIRE OpenORG ID\", \"OpenAIRE OpenORG ID LINK\", \"openorg_id\") or None,\n",
    "            \"main_datasource_id\": _pick(row, \"OpenAIRE Data Source ID (Main/CRIS)\", \"OpenAIRE Data Source ID (Main/CRIS) LINK\", \"main_datasource_id\") or None,\n",
    "            \"secondary_datasource_id\": _pick(row, \"OpenAIRE Data Source (Secondary/Repository)\", \"OpenAIRE Data Source (Secondary/Repository) LINK\", \"secondary_datasource_id\") or None,\n",
    "        }\n",
    "        # Special handling for OpenDOAR IDs\n",
    "        if entry[\"main_datasource_id\"] and entry[\"main_datasource_id\"].startswith(\"opendoar\"):\n",
    "            if not entry[\"secondary_datasource_id\"]:\n",
    "                entry[\"secondary_datasource_id\"] = entry[\"main_datasource_id\"]\n",
    "                entry[\"main_datasource_id\"] = None\n",
    "        # Append the parsed entry to the list\n",
    "        parsed.append(entry)\n",
    "\n",
    "    # Return the parsed university entries\n",
    "    return parsed\n",
    "\n",
    "# Load the NL organizations reference table and parse universities\n",
    "nl_orgs_df = load_university_table(TABLE_PATH, NL_ORGS_SHEET_NAME)\n",
    "# Parse universities from the loaded DataFrame\n",
    "universities = parse_university_rows(nl_orgs_df)\n",
    "# Display the number of parsed universities\n",
    "print(f\"Parsed {len(universities)} universities from the reference table (loaded from {TABLE_PATH}).\")\n",
    "# Convert the list of universities to a DataFrame for display\n",
    "universities_df = pd.DataFrame(universities)\n",
    "# Display the DataFrame of universities\n",
    "universities_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece3bd2d",
   "metadata": {},
   "source": [
    "## 3. Helper functions for the Graph API\n",
    "These functions wrap the REST requests and centralise filter construction per scenario. Each call prints nothing by default so we can reuse them freely in later cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce1e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the different scenarios for metrics collection.\n",
    "# Each scenario includes a key, label, ID field, and description.\n",
    "# For each scenario, we will build appropriate filters to query the OpenAIRE Graph API.\n",
    "# For example, we can collect metrics based on organization affiliation,\n",
    "# main data source, or secondary repository.\n",
    "# Each scenario will have its own set of filters.\n",
    "\n",
    "SCENARIO_DEFS = [\n",
    "    {\n",
    "        \"key\": \"organization\",\n",
    "        \"label\": \"A. OpenORG affiliation\",\n",
    "        \"id_field\": \"OpenAIRE_ORG_ID\",\n",
    "        \"description\": \"Publications affiliated to the university (OpenAIRE OpenORG ID)\",\n",
    "    },\n",
    "    {\n",
    "        \"key\": \"main_datasource\",\n",
    "        \"label\": \"B. Main/CRIS data source\",\n",
    "        \"id_field\": \"main_datasource_id\",\n",
    "        \"description\": \"Publications collected from the main CRIS data source\",\n",
    "    },\n",
    "    {\n",
    "        \"key\": \"secondary_datasource\",\n",
    "        \"label\": \"C. Secondary repository\",\n",
    "        \"id_field\": \"secondary_datasource_id\",\n",
    "        \"description\": \"Publications collected from the secondary / repository data source\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Define filter builders for each scenario.\n",
    "# Each builder function takes an entity ID and returns a dictionary of filters\n",
    "# for projects, data sources, and research products.\n",
    "#   - The filters are used to query the OpenAIRE Graph API for relevant metrics.\n",
    "\n",
    "FILTER_BUILDERS = {\n",
    "    \"organization\": lambda entity_id: {\n",
    "        \"projects\": {\"relOrganizationId\": entity_id},\n",
    "        \"dataSources\": {\"relOrganizationId\": entity_id},\n",
    "        \"researchProducts\": {\"relOrganizationId\": entity_id},\n",
    "    },\n",
    "    \"main_datasource\": lambda entity_id: {\n",
    "        \"projects\": {\"relCollectedFromDatasourceId\": entity_id},\n",
    "        \"dataSources\": {\"id\": entity_id},\n",
    "        \"researchProducts\": {\"relCollectedFromDatasourceId\": entity_id},\n",
    "    },\n",
    "    \"secondary_datasource\": lambda entity_id: {\n",
    "        \"projects\": {\"relCollectedFromDatasourceId\": entity_id},\n",
    "        \"dataSources\": {\"id\": entity_id},\n",
    "        \"researchProducts\": {\"relCollectedFromDatasourceId\": entity_id},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define an empty metrics dictionary for collecting results.\n",
    "EMPTY_METRICS = {metric: None for metric in METRIC_ORDER}\n",
    "\n",
    "# Obtain a cached OpenAIRE access token, refreshing it when needed.\n",
    "def obtain_access_token() -> str:\n",
    "    \"\"\"Return a cached OpenAIRE access token, refreshing it when needed.\"\"\"\n",
    "    global _access_token, _access_token_expiry\n",
    "    now = time.time()\n",
    "    if _access_token and now < _access_token_expiry:\n",
    "        return _access_token\n",
    "\n",
    "    response = requests.post(\n",
    "        TOKEN_URL,\n",
    "        data={\"grant_type\": \"client_credentials\"},\n",
    "        auth=(CLIENT_ID, CLIENT_SECRET),\n",
    "        headers={\"User-Agent\": API_USER_AGENT},\n",
    "        timeout=60,\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    payload = response.json()\n",
    "    token = payload.get(\"access_token\")\n",
    "    if not token:\n",
    "        raise RuntimeError(\"OpenAIRE token response did not include an access_token.\")\n",
    "    expires_in = int(payload.get(\"expires_in\", 3600))\n",
    "    _access_token = token\n",
    "    _access_token_expiry = now + max(expires_in - TOKEN_REFRESH_BUFFER, 0)\n",
    "    return _access_token\n",
    "\n",
    "# Invoke the OpenAIRE Graph API and return the decoded JSON payload.\n",
    "def call_graph_api(path: str, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Invoke the OpenAIRE Graph API and return the decoded JSON payload.\"\"\"\n",
    "    url = f\"{BASE_URL}{path}\"\n",
    "    effective_params = dict(params or {})\n",
    "    effective_params.setdefault(\"page\", 1)\n",
    "    effective_params.setdefault(\"pageSize\", 1)\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": API_USER_AGENT,\n",
    "        \"Authorization\": f\"Bearer {obtain_access_token()}\",\n",
    "    }\n",
    "\n",
    "    response = requests.get(\n",
    "        url,\n",
    "        params=effective_params,\n",
    "        headers=headers,\n",
    "        timeout=60,\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    time.sleep(API_PAUSE_SECONDS)\n",
    "    return response.json()\n",
    "\n",
    "# Retrieve the total number of matching records for the supplied endpoint.\n",
    "def fetch_num_found(path: str, params: Dict[str, Any]) -> Optional[int]:\n",
    "    \"\"\"Return the total number of matching records for the supplied endpoint.\"\"\"\n",
    "    payload = call_graph_api(path, params)\n",
    "    header = payload.get(\"header\", {})\n",
    "    num_found = header.get(\"numFound\")\n",
    "    return int(num_found) if num_found is not None else None\n",
    "\n",
    "# Build filters for the given scenario and entity ID.\n",
    "def build_filters(scenario_key: str, entity_id: str) -> Dict[str, Dict[str, Any]]:\n",
    "    builder = FILTER_BUILDERS[scenario_key]\n",
    "    return {name: dict(filters) for name, filters in builder(entity_id).items()}\n",
    "\n",
    "\n",
    "# Collect metrics for a given scenario and entity ID.\n",
    "def collect_metrics(scenario_key: str, entity_id: Optional[str]) -> Dict[str, Optional[int]]:\n",
    "    if not entity_id:\n",
    "        return deepcopy(EMPTY_METRICS)\n",
    "\n",
    "    filters = build_filters(scenario_key, entity_id)\n",
    "    results: Dict[str, Optional[int]] = {}\n",
    "\n",
    "    results[\"Funding / Projects\"] = fetch_num_found(\"/v1/projects\", filters[\"projects\"])\n",
    "    results[\"Data sources\"] = fetch_num_found(\"/v1/dataSources\", filters[\"dataSources\"])\n",
    "\n",
    "    for rp_type, label in PRODUCT_TYPE_LABELS.items():\n",
    "        rp_params = dict(filters[\"researchProducts\"], type=rp_type)\n",
    "        results[label] = fetch_num_found(\"/v2/researchProducts\", rp_params)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6634bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "# Cache OpenAIRE ID lookups to avoid redundant API calls\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def _fetch_openorg_id(ror_url: str) -> Optional[str]:\n",
    "    # Query the OpenAIRE Graph API for the organization with the given ROR URL\n",
    "    try:\n",
    "        payload = call_graph_api(\"/v1/organizations\", {\"pid\": ror_url})\n",
    "    except requests.RequestException as exc:\n",
    "        print(f\"Failed to fetch OpenAIRE ID for {ror_url}: {exc}\")\n",
    "        return None\n",
    "\n",
    "    # Extract the OpenAIRE organization ID from the response\n",
    "    results = payload.get(\"results\") or []\n",
    "    for item in results:\n",
    "        openorg_id = item.get(\"id\")\n",
    "        if openorg_id:\n",
    "            return openorg_id\n",
    "    return None\n",
    "\n",
    "# Look up the OpenAIRE organization identifier using a ROR ID or URL.\n",
    "\n",
    "def fetch_openorg_id_for_ror(ror_value: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"Look up the OpenAIRE organization identifier using a ROR ID or URL.\"\"\"\n",
    "    if not ror_value:\n",
    "        return None\n",
    "    # Normalize the ROR value to a URL\n",
    "    ror_url = normalise_ror_link(ror_value)\n",
    "    if not ror_url:\n",
    "        return None\n",
    "    # Look up the OpenAIRE organization ID using the normalized ROR URL\n",
    "    return _fetch_openorg_id(ror_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b919d9",
   "metadata": {},
   "source": [
    "## 4. Test for one university\n",
    "\n",
    "Run a quick test for the first university in the list to fetch the \n",
    "  OpenORG affiliation,\n",
    "  and the Number of: \n",
    "  Funding / Projects,\n",
    "  Data sources, \n",
    "  Products: Total,\n",
    "  Products: Publications, \n",
    "  Products: Research data, \n",
    "  Products: Research software,\n",
    "  Products: Other research products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: collect and display metrics for the first university in the list\n",
    "test_university = universities[0]\n",
    "print(f\"Testing metrics for: {test_university['name']}\")\n",
    "\n",
    "# Determine the OpenAIRE organization identifier\n",
    "identifier = test_university.get(\"OpenAIRE_ORG_ID\")\n",
    "if not identifier:\n",
    "    # Try to resolve it via the ROR ID/link\n",
    "    resolved = fetch_openorg_id_for_ror(test_university.get(\"ROR_LINK\") or test_university.get(\"ROR\"))\n",
    "    identifier = resolved\n",
    "\n",
    "# Collect and display metrics    \n",
    "print(f\"\\nOpenAIRE ID: {identifier}\")\n",
    "print(\"\\nMetrics:\")\n",
    "# Collect metrics for the organization\n",
    "metrics = collect_metrics(\"organization\", identifier)\n",
    "\n",
    "# Calculate total research products\n",
    "research_products_total = sum(\n",
    "    metrics.get(product_type, 0) or 0 \n",
    "    for product_type in [\"Publications\", \"Research data\", \"Research software\", \"Other research products\"]\n",
    ")\n",
    "print(\"Total Research Products:\", research_products_total)\n",
    "print(\"\\nBreakdown by type:\")\n",
    "\n",
    "# Display the collected metrics\n",
    "for metric, count in metrics.items():\n",
    "    print(f\"{metric}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b427e134",
   "metadata": {},
   "source": [
    "## 5. Collect metrics for all universities\n",
    "Loop through every organisation, ensure an OpenAIRE identifier is available, and add this information to new columns. and write to file nl_orgs_openaire.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc7ca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Enrich the NL organizations table with OpenAIRE IDs and metrics\n",
    "\n",
    "metric_columns = [\n",
    "    \"Data sources\",\n",
    "    \"Total Research Products\",\n",
    "    \"Publications\",\n",
    "    \"Research data\",\n",
    "    \"Research software\",\n",
    "    \"Other research products\",\n",
    "]\n",
    "\n",
    "output_path = DATA_DIR / \"nl_orgs_openaire.xlsx\"\n",
    "checkpoint_path = DATA_DIR / \"nl_orgs_openaire.tmp.xlsx\"\n",
    "save_every = 5  # save every N rows\n",
    "max_workers = 6  # number of concurrent workers\n",
    "\n",
    "if output_path.exists():\n",
    "    enriched_df = pd.read_excel(output_path)\n",
    "    for column in metric_columns:\n",
    "        if column not in enriched_df.columns:\n",
    "            enriched_df[column] = pd.NA\n",
    "    print(f\"Loaded existing enriched dataset from {output_path}\")\n",
    "else:\n",
    "    enriched_df = universities_df.copy()\n",
    "    for column in metric_columns:\n",
    "        if column not in enriched_df.columns:\n",
    "            enriched_df[column] = pd.NA\n",
    "\n",
    "    def enrich_row(idx: int) -> tuple[int, dict[str, Any]]:\n",
    "        row = enriched_df.loc[idx]\n",
    "        identifier = row.get(\"OpenAIRE_ORG_ID\")\n",
    "        added_id = False\n",
    "        if not identifier:\n",
    "            identifier = fetch_openorg_id_for_ror(row.get(\"ROR_LINK\") or row.get(\"ROR\"))\n",
    "            if identifier:\n",
    "                added_id = True\n",
    "\n",
    "        metrics = collect_metrics(\"organization\", identifier)\n",
    "        result = {\"OpenAIRE_ORG_ID\": identifier}\n",
    "\n",
    "        if identifier and metrics:\n",
    "            total_products = sum((metrics.get(label) or 0) for label in metric_columns[2:])\n",
    "            for column in metric_columns:\n",
    "                result[column] = total_products if column == \"Total Research Products\" else metrics.get(column)\n",
    "        else:\n",
    "            for column in metric_columns:\n",
    "                result[column] = pd.NA\n",
    "\n",
    "        result[\"added_id\"] = added_id\n",
    "        return idx, result\n",
    "\n",
    "    progress = tqdm(enriched_df.index, desc=\"Enriching OpenAIRE IDs & metrics\", unit=\"org\")\n",
    "    enriched_count = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(enrich_row, idx): idx for idx in enriched_df.index}\n",
    "        for processed, future in enumerate(as_completed(futures), start=1):\n",
    "            idx, result = future.result()\n",
    "            enriched_df.at[idx, \"OpenAIRE_ORG_ID\"] = result[\"OpenAIRE_ORG_ID\"]\n",
    "            for column in metric_columns:\n",
    "                enriched_df.at[idx, column] = result[column]\n",
    "            if result.get(\"added_id\"):\n",
    "                enriched_count += 1\n",
    "            progress.update(1)\n",
    "            if processed % save_every == 0:\n",
    "                enriched_df.to_excel(checkpoint_path, index=False)\n",
    "                progress.set_postfix(saved_rows=processed)\n",
    "\n",
    "    progress.close()\n",
    "    enriched_df.to_excel(output_path, index=False)\n",
    "    print(f\"Added OpenAIRE IDs for {enriched_count} organizations\")\n",
    "    print(f\"Saved enriched data to {output_path}\")\n",
    "    if checkpoint_path.exists():\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "enriched_df.head()\n",
    "\n",
    "missing_ids = enriched_df[enriched_df[\"OpenAIRE_ORG_ID\"].isna() | (enriched_df[\"OpenAIRE_ORG_ID\"] == \"\")]\n",
    "if not missing_ids.empty:\n",
    "    print(\"Organizations still missing OpenAIRE IDs:\")\n",
    "    for name in missing_ids['name']:\n",
    "        print(f\"- {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75c17df",
   "metadata": {},
   "source": [
    "## 6. Visualise total research products per organisation\n",
    "Convert the enriched metrics to numeric form and plot the total research products per organisation (sorted descending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b4487",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [\n",
    "    \"Total Research Products\",\n",
    "    \"Publications\",\n",
    "    \"Research data\",\n",
    "    \"Research software\",\n",
    "    \"Other research products\",\n",
    "]\n",
    "\n",
    "for column in numeric_columns:\n",
    "    enriched_df[column] = pd.to_numeric(enriched_df[column], errors=\"coerce\")\n",
    "\n",
    "plot_df = enriched_df.sort_values(\"Total Research Products\", ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 20))\n",
    "has_missing_id = plot_df[\"OpenAIRE_ORG_ID\"].isna()\n",
    "bars = ax.barh(range(len(plot_df)), plot_df[\"Total Research Products\"])\n",
    "for idx, missing in enumerate(has_missing_id):\n",
    "    bars[idx].set_color(\"#cccccc\" if missing else \"#1f77b4\")\n",
    "\n",
    "ax.set_yticks(range(len(plot_df)))\n",
    "ax.set_yticklabels(plot_df[\"name\"])\n",
    "for idx, label in enumerate(ax.get_yticklabels()):\n",
    "    label.set_color(\"#cccccc\" if has_missing_id.iloc[idx] else \"black\")\n",
    "\n",
    "ax.set_xlabel(\"Total research products\")\n",
    "ax.set_ylabel(\"Organisation\")\n",
    "ax.set_title(\"Total research products per organisation\")\n",
    "fig.tight_layout()\n",
    "org_chart_path = IMG_DIR / \"org_total_products.png\"\n",
    "fig.savefig(org_chart_path, dpi=200, bbox_inches=\"tight\")\n",
    "print(f\"Saved organisation totals chart to {org_chart_path}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035456d5",
   "metadata": {},
   "source": [
    "## 7. Fetch data source metadata\n",
    "Retrieve every OpenAIRE data source linked to the enriched organisations and cache the registry details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c9a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Fetch and store OpenAIRE data source metadata for NL organizations\n",
    "\n",
    "datasource_columns = [\n",
    "    \"OpenAIRE_ORG_ID\",\n",
    "    \"OpenAIRE_DataSource_ID\",\n",
    "    \"Name\",\n",
    "    \"Type\",\n",
    "    \"websiteUrl\",\n",
    "    \"OAI-endpoint\",\n",
    "    \"supports_NL-DIDL\",\n",
    "    \"support_OAI-DC\",\n",
    "    \"support_OAI-openaire\",\n",
    "    \"supports_RIOXX\",\n",
    "    \"support_OpenAIRE-CERIF\",\n",
    "    \"openaireCompatibility\",\n",
    "    \"Last_Indexed_Date\",\n",
    "    \"dateOfValidation\",\n",
    "]\n",
    "\n",
    "datasources_path = DATA_DIR / \"nl_orgs_openaire_datasources.xlsx\"\n",
    "\n",
    "if datasources_path.exists():\n",
    "    datasources_df = pd.read_excel(datasources_path)\n",
    "    print(f\"Loaded existing datasource metadata from {datasources_path}\")\n",
    "else:\n",
    "    def _parse_datasource_record(org_id: Optional[str], item: Dict[str, Any]) -> dict[str, Any]:\n",
    "        name = item.get(\"officialName\") or item.get(\"englishName\")\n",
    "        ds_type = (item.get(\"type\") or {}).get(\"value\")\n",
    "        return {\n",
    "            \"OpenAIRE_ORG_ID\": org_id,\n",
    "            \"OpenAIRE_DataSource_ID\": item.get(\"id\"),\n",
    "            \"Name\": name,\n",
    "            \"Type\": ds_type,\n",
    "            \"websiteUrl\": item.get(\"websiteUrl\"),\n",
    "            \"OAI-endpoint\": None,\n",
    "            \"supports_NL-DIDL\": None,\n",
    "            \"support_OAI-DC\": None,\n",
    "            \"support_OAI-openaire\": None,\n",
    "            \"supports_RIOXX\": None,\n",
    "            \"support_OpenAIRE-CERIF\": None,\n",
    "            \"openaireCompatibility\": item.get(\"openaireCompatibility\"),\n",
    "            \"Last_Indexed_Date\": item.get(\"lastIndexedDate\") or item.get(\"lastIndexDate\"),\n",
    "            \"dateOfValidation\": item.get(\"dateOfValidation\"),\n",
    "        }\n",
    "\n",
    "    def fetch_datasources_for_org(org_entry: dict[str, Any]) -> list[dict[str, Any]]:\n",
    "        org_id = org_entry.get(\"OpenAIRE_ORG_ID\")\n",
    "        if not org_id:\n",
    "            return []\n",
    "        records: list[dict[str, Any]] = []\n",
    "        page = 1\n",
    "        page_size = 100\n",
    "        while True:\n",
    "            payload = call_graph_api(\n",
    "                \"/v1/dataSources\",\n",
    "                {\n",
    "                    \"relOrganizationId\": org_id,\n",
    "                    \"page\": page,\n",
    "                    \"pageSize\": page_size,\n",
    "                },\n",
    "            )\n",
    "            results = payload.get(\"results\") or []\n",
    "            for item in results:\n",
    "                records.append(_parse_datasource_record(org_id, item))\n",
    "            header = payload.get(\"header\") or {}\n",
    "            num_found = header.get(\"numFound\", 0)\n",
    "            if page * page_size >= num_found or not results:\n",
    "                break\n",
    "            page += 1\n",
    "        return records\n",
    "\n",
    "    datasource_records: list[dict[str, Any]] = []\n",
    "    org_entries = enriched_df.to_dict(\"records\")\n",
    "    with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "        futures = {executor.submit(fetch_datasources_for_org, entry): entry for entry in org_entries}\n",
    "        for future in tqdm(\n",
    "            as_completed(futures),\n",
    "            total=len(org_entries),\n",
    "            desc=\"Fetching data sources\",\n",
    "            unit=\"org\",\n",
    "        ):\n",
    "            datasource_records.extend(future.result())\n",
    "\n",
    "    datasources_df = pd.DataFrame(datasource_records, columns=datasource_columns)\n",
    "    datasources_df.to_excel(datasources_path, index=False)\n",
    "    print(f\"Saved {len(datasources_df)} data source rows to {datasources_path}\")\n",
    "\n",
    "datasources_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3254bb",
   "metadata": {},
   "source": [
    "## 8. Capture data source content volumes\n",
    "Collect fresh numFound counts per data source (total and by product type) and store the snapshot with today's date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ad59bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8. Collect and store numFound snapshots for data sources\n",
    "\n",
    "snapshot_date = datetime.utcnow().date().isoformat()\n",
    "snapshot_path = DATA_DIR / f\"nl_orgs_openaire_datasources_numFound_{snapshot_date}.xlsx\"\n",
    "\n",
    "if snapshot_path.exists():\n",
    "    datasource_metrics_df = pd.read_excel(snapshot_path)\n",
    "    print(f\"Loaded existing numFound snapshot for {snapshot_date} from {snapshot_path}\")\n",
    "elif datasources_df.empty:\n",
    "    print(\"No data sources available; skipping numFound snapshot.\")\n",
    "    datasource_metrics_df = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"OpenAIRE_DataSource_ID\",\n",
    "            \"Name\",\n",
    "            \"Total Research Products\",\n",
    "            *PRODUCT_TYPE_LABELS.values(),\n",
    "            \"date_retrieved\",\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    def collect_datasource_counts(row: pd.Series) -> dict[str, Any]:\n",
    "        datasource_id = getattr(row, \"OpenAIRE_DataSource_ID\", None)\n",
    "        if not datasource_id:\n",
    "            return {}\n",
    "        metrics: dict[str, Any] = {\n",
    "            \"OpenAIRE_DataSource_ID\": datasource_id,\n",
    "            \"Name\": getattr(row, \"Name\", None),\n",
    "            \"date_retrieved\": snapshot_date,\n",
    "        }\n",
    "        total_params = {\"relCollectedFromDatasourceId\": datasource_id}\n",
    "        metrics[\"Total Research Products\"] = fetch_num_found(\n",
    "            \"/v2/researchProducts\", total_params\n",
    "        )\n",
    "        for rp_type, label in PRODUCT_TYPE_LABELS.items():\n",
    "            rp_params = {\"relCollectedFromDatasourceId\": datasource_id, \"type\": rp_type}\n",
    "            metrics[label] = fetch_num_found(\"/v2/researchProducts\", rp_params)\n",
    "        return metrics\n",
    "\n",
    "    datasource_metrics: list[dict[str, Any]] = []\n",
    "    with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "        futures = {\n",
    "            executor.submit(collect_datasource_counts, row): row\n",
    "            for row in datasources_df.itertuples(index=False)\n",
    "        }\n",
    "        for future in tqdm(\n",
    "            as_completed(futures),\n",
    "            total=len(futures),\n",
    "            desc=\"Collecting numFound\",\n",
    "            unit=\"datasource\",\n",
    "        ):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                datasource_metrics.append(result)\n",
    "\n",
    "    datasource_metrics_df = pd.DataFrame(\n",
    "        datasource_metrics,\n",
    "        columns=[\n",
    "            \"OpenAIRE_DataSource_ID\",\n",
    "            \"Name\",\n",
    "            \"Total Research Products\",\n",
    "            *PRODUCT_TYPE_LABELS.values(),\n",
    "            \"date_retrieved\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    datasource_metrics_df.to_excel(snapshot_path, index=False)\n",
    "    print(\n",
    "        f\"Saved snapshot with {len(datasource_metrics_df)} data sources to {snapshot_path}\"\n",
    "    )\n",
    "\n",
    "datasource_metrics_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be14dd",
   "metadata": {},
   "source": [
    "## 9. Append snapshot to parquet history\n",
    "Keep a cumulative parquet log so repeated snapshots form a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699e34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 9. Append snapshot to historical log\n",
    "\n",
    "history_path = DATA_DIR / \"nl_orgs_openaire_datasources_numFound_history.xlsx\"\n",
    "history_columns = [\n",
    "    \"OpenAIRE_DataSource_ID\",\n",
    "    \"Name\",\n",
    "    \"Total Research Products\",\n",
    "    *PRODUCT_TYPE_LABELS.values(),\n",
    "    \"date_retrieved\",\n",
    "]\n",
    "\n",
    "if history_path.exists():\n",
    "    historical_df = pd.read_excel(history_path)\n",
    "    print(f\"Loaded historical log from {history_path}\")\n",
    "else:\n",
    "    historical_df = pd.DataFrame(columns=history_columns)\n",
    "    print(\"No historical log found; a new one will be created if data is appended.\")\n",
    "\n",
    "if datasource_metrics_df.empty:\n",
    "    print(\"No snapshot data to append.\")\n",
    "else:\n",
    "    snapshot_date_value = None\n",
    "    if \"date_retrieved\" in datasource_metrics_df.columns and not datasource_metrics_df[\"date_retrieved\"].isna().all():\n",
    "        snapshot_date_value = str(datasource_metrics_df[\"date_retrieved\"].iloc[0])\n",
    "\n",
    "    already_recorded = (\n",
    "        snapshot_date_value is not None\n",
    "        and not historical_df.empty\n",
    "        and \"date_retrieved\" in historical_df.columns\n",
    "        and snapshot_date_value in historical_df[\"date_retrieved\"].astype(str).values\n",
    "    )\n",
    "\n",
    "    if already_recorded:\n",
    "        print(f\"Snapshot for {snapshot_date_value} already recorded; skipping append.\")\n",
    "    else:\n",
    "        combined_df = (\n",
    "            pd.concat([historical_df, datasource_metrics_df], ignore_index=True)\n",
    "            if not historical_df.empty\n",
    "            else datasource_metrics_df.copy()\n",
    "        )\n",
    "        for column in [\n",
    "            \"Total Research Products\",\n",
    "            *PRODUCT_TYPE_LABELS.values(),\n",
    "        ]:\n",
    "            if column in combined_df.columns:\n",
    "                combined_df[column] = pd.to_numeric(combined_df[column], errors=\"coerce\")\n",
    "\n",
    "        combined_df.to_excel(history_path, index=False)\n",
    "        historical_df = combined_df\n",
    "        print(\n",
    "            f\"History now contains {len(historical_df)} rows at {history_path}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b33635f",
   "metadata": {},
   "source": [
    "## 10. Visualise data source totals\n",
    "Bar chart of the latest total research products per data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e5528",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = DATA_DIR / \"nl_orgs_openaire_datasources_numFound_history.xlsx\"\n",
    "datasources_path = DATA_DIR / \"nl_orgs_openaire_datasources.xlsx\"\n",
    "if not history_path.exists():\n",
    "    print(f\"History file {history_path} not found.\")\n",
    "else:\n",
    "    history_df = pd.read_excel(history_path)\n",
    "    numeric_columns = [\n",
    "        \"Total Research Products\",\n",
    "        *PRODUCT_TYPE_LABELS.values(),\n",
    "    ]\n",
    "    for column in numeric_columns:\n",
    "        if column in history_df.columns:\n",
    "            history_df[column] = pd.to_numeric(history_df[column], errors=\"coerce\")\n",
    "\n",
    "    history_df[\"date_retrieved\"] = pd.to_datetime(history_df[\"date_retrieved\"], errors=\"coerce\")\n",
    "    latest_date = history_df[\"date_retrieved\"].max()\n",
    "    latest_df = history_df[history_df[\"date_retrieved\"] == latest_date].copy()\n",
    "\n",
    "    if latest_df.empty:\n",
    "        print(\"No rows for the latest snapshot date.\")\n",
    "    else:\n",
    "        if \"Type\" not in latest_df.columns or latest_df[\"Type\"].isna().all():\n",
    "            if 'datasources_df' in globals():\n",
    "                type_lookup = datasources_df[[\"OpenAIRE_DataSource_ID\", \"Type\"]].copy()\n",
    "            elif datasources_path.exists():\n",
    "                type_lookup = pd.read_excel(datasources_path)[\n",
    "                    [\"OpenAIRE_DataSource_ID\", \"Type\"]\n",
    "                ].copy()\n",
    "            else:\n",
    "                type_lookup = pd.DataFrame(columns=[\"OpenAIRE_DataSource_ID\", \"Type\"])\n",
    "\n",
    "            latest_df = latest_df.merge(\n",
    "                type_lookup,\n",
    "                on=\"OpenAIRE_DataSource_ID\",\n",
    "                how=\"left\",\n",
    "                suffixes=(None, \"_ds\"),\n",
    "            )\n",
    "            if \"Type_ds\" in latest_df.columns:\n",
    "                latest_df[\"Type\"] = latest_df[\"Type\"].fillna(latest_df[\"Type_ds\"])\n",
    "                latest_df = latest_df.drop(columns=[\"Type_ds\"])\n",
    "\n",
    "        zero_mask = latest_df[\"Total Research Products\"].fillna(0) <= 0\n",
    "        zero_total_df = latest_df[zero_mask].copy()\n",
    "        if not zero_total_df.empty:\n",
    "            print(\"Datasources with zero total research products (excluded from chart):\")\n",
    "            for _, row in zero_total_df.iterrows():\n",
    "                print(f\" - {row.get('Name', 'Unknown')} ({row.get('OpenAIRE_DataSource_ID')})\")\n",
    "\n",
    "        latest_df = latest_df.loc[~zero_mask].copy()\n",
    "        if latest_df.empty:\n",
    "            print(\"All datasources reported zero totals; skipping chart.\")\n",
    "        else:\n",
    "            latest_df = latest_df.sort_values(\"Total Research Products\", ascending=False)\n",
    "            max_label_length = 40\n",
    "            display_names = (\n",
    "                latest_df[\"Name\"].fillna(\"Unknown\")\n",
    "                .astype(str)\n",
    "                .apply(lambda text: text if len(text) <= max_label_length else text[: max_label_length - 1] + \"…\")\n",
    "            )\n",
    "\n",
    "            type_series = latest_df[\"Type\"].fillna(\"Unknown\")\n",
    "            unique_types = type_series.unique()\n",
    "            cmap = plt.cm.get_cmap(\"tab20\", max(len(unique_types), 1))\n",
    "            color_map = {t: cmap(i) for i, t in enumerate(unique_types)}\n",
    "            colors = type_series.map(color_map)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(12, 10))\n",
    "            ax.barh(display_names, latest_df[\"Total Research Products\"], color=colors)\n",
    "            ax.set_title(\n",
    "                f\"Total research products per data source (snapshot {latest_date.date()})\"\n",
    "            )\n",
    "            ax.set_xlabel(\"Total research products\")\n",
    "            ax.set_ylabel(\"Data source\")\n",
    "            ax.invert_yaxis()\n",
    "\n",
    "            handles = [plt.Rectangle((0, 0), 1, 1, color=color_map[t]) for t in unique_types]\n",
    "            ax.legend(\n",
    "                handles,\n",
    "                unique_types,\n",
    "                title=\"Data source type\",\n",
    "                loc=\"upper center\",\n",
    "                bbox_to_anchor=(0.5, -0.12),\n",
    "                ncol=3,\n",
    "            )\n",
    "            fig.tight_layout()\n",
    "            chart_path = IMG_DIR / \"datasource_totals_latest.png\"\n",
    "            fig.savefig(chart_path, dpi=200, bbox_inches=\"tight\")\n",
    "            print(f\"Saved latest data source totals chart to {chart_path}\")\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5204e1ff",
   "metadata": {},
   "source": [
    "## 11. Compare latest vs previous snapshot\n",
    "Show side-by-side totals for the latest two snapshots per data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76950aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = DATA_DIR / \"nl_orgs_openaire_datasources_numFound_history.xlsx\"\n",
    "datasources_path = DATA_DIR / \"nl_orgs_openaire_datasources.xlsx\"\n",
    "if not history_path.exists():\n",
    "    print(f\"History file {history_path} not found.\")\n",
    "else:\n",
    "    history_df = pd.read_excel(history_path)\n",
    "    numeric_columns = [\n",
    "        \"Total Research Products\",\n",
    "        *PRODUCT_TYPE_LABELS.values(),\n",
    "    ]\n",
    "    for column in numeric_columns:\n",
    "        if column in history_df.columns:\n",
    "            history_df[column] = pd.to_numeric(history_df[column], errors=\"coerce\")\n",
    "\n",
    "    history_df[\"date_retrieved\"] = pd.to_datetime(history_df[\"date_retrieved\"], errors=\"coerce\")\n",
    "    unique_dates = sorted(history_df[\"date_retrieved\"].dropna().unique())\n",
    "    if len(unique_dates) < 2:\n",
    "        print(\"Not enough snapshots to compare (need at least two dates).\")\n",
    "    else:\n",
    "        latest_date = unique_dates[-1]\n",
    "        previous_date = unique_dates[-2]\n",
    "        latest_df = history_df[history_df[\"date_retrieved\"] == latest_date].copy()\n",
    "        previous_df = history_df[history_df[\"date_retrieved\"] == previous_date].copy()\n",
    "\n",
    "        if 'datasources_df' in globals():\n",
    "            type_lookup = datasources_df[[\"OpenAIRE_DataSource_ID\", \"Type\"]].copy()\n",
    "        elif datasources_path.exists():\n",
    "            type_lookup = pd.read_excel(datasources_path)[\n",
    "                [\"OpenAIRE_DataSource_ID\", \"Type\"]\n",
    "            ].copy()\n",
    "        else:\n",
    "            type_lookup = pd.DataFrame(columns=[\"OpenAIRE_DataSource_ID\", \"Type\"])\n",
    "\n",
    "        def attach_type(df: pd.DataFrame) -> pd.DataFrame:\n",
    "            if \"Type\" in df.columns and df[\"Type\"].notna().any():\n",
    "                return df\n",
    "            merged = df.merge(\n",
    "                type_lookup,\n",
    "                on=\"OpenAIRE_DataSource_ID\",\n",
    "                how=\"left\",\n",
    "                suffixes=(None, \"_ds\"),\n",
    "            )\n",
    "            if \"Type_ds\" in merged.columns:\n",
    "                merged[\"Type\"] = merged[\"Type\"].fillna(merged[\"Type_ds\"])\n",
    "                merged = merged.drop(columns=[\"Type_ds\"])\n",
    "            return merged\n",
    "\n",
    "        latest_df = attach_type(latest_df)\n",
    "        previous_df = attach_type(previous_df)\n",
    "\n",
    "        combined = latest_df[[\n",
    "            \"OpenAIRE_DataSource_ID\",\n",
    "            \"Name\",\n",
    "            \"Type\",\n",
    "            \"Total Research Products\",\n",
    "        ]].rename(columns={\"Total Research Products\": \"Total Research Products_latest\"})\n",
    "\n",
    "        combined = combined.merge(\n",
    "            previous_df[[\"OpenAIRE_DataSource_ID\", \"Total Research Products\"]]\n",
    "            .rename(columns={\"Total Research Products\": \"Total Research Products_previous\"}),\n",
    "            on=\"OpenAIRE_DataSource_ID\",\n",
    "            how=\"outer\",\n",
    "        )\n",
    "\n",
    "        combined[\"Name\"] = combined[\"Name\"].fillna(\"Unknown\")\n",
    "        combined[\"Type\"] = combined[\"Type\"].fillna(\"Unknown\")\n",
    "        combined = combined.fillna({\n",
    "            \"Total Research Products_latest\": 0,\n",
    "            \"Total Research Products_previous\": 0,\n",
    "        })\n",
    "\n",
    "        zero_mask = combined[\"Total Research Products_latest\"].fillna(0) <= 0\n",
    "        zero_total_df = combined[zero_mask].copy()\n",
    "        if not zero_total_df.empty:\n",
    "            print(\"Datasources with zero total research products (excluded from comparison chart):\")\n",
    "            for _, row in zero_total_df.iterrows():\n",
    "                print(f\" - {row.get('Name', 'Unknown')} ({row.get('OpenAIRE_DataSource_ID')})\")\n",
    "\n",
    "        combined = combined.loc[~zero_mask].copy()\n",
    "        if combined.empty:\n",
    "            print(\"No datasources with non-zero totals available for comparison chart.\")\n",
    "        else:\n",
    "            combined = combined.sort_values(\"Total Research Products_latest\", ascending=False)\n",
    "            max_label_length = 40\n",
    "            display_names = (\n",
    "                combined[\"Name\"].astype(str)\n",
    "                .apply(lambda text: text if len(text) <= max_label_length else text[: max_label_length - 1] + \"…\")\n",
    "            )\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(12, 30))\n",
    "            y_positions = range(len(combined))\n",
    "            bar_height = 0.35\n",
    "            ax.barh(\n",
    "                [y + bar_height / 2 for y in y_positions],\n",
    "                combined[\"Total Research Products_latest\"],\n",
    "                height=bar_height,\n",
    "                color=\"#1a9850\",\n",
    "                label=f\"Latest ({latest_date.date()})\",\n",
    "            )\n",
    "            ax.barh(\n",
    "                [y - bar_height / 2 for y in y_positions],\n",
    "                combined[\"Total Research Products_previous\"],\n",
    "                height=bar_height,\n",
    "                color=\"#b8e186\",\n",
    "                label=f\"Previous ({previous_date.date()})\",\n",
    "            )\n",
    "\n",
    "            ax.set_yticks(list(y_positions))\n",
    "            ax.set_yticklabels(display_names)\n",
    "            ax.set_xlabel(\"Total research products\")\n",
    "            ax.set_title(\"Latest vs previous total research products per data source\")\n",
    "            ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.08), ncol=2)\n",
    "            ax.invert_yaxis()\n",
    "            fig.tight_layout()\n",
    "            compare_path = IMG_DIR / \"datasource_totals_compare.png\"\n",
    "            fig.savefig(compare_path, dpi=200, bbox_inches=\"tight\")\n",
    "            print(f\"Saved comparison chart to {compare_path}\")\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7e5f7a",
   "metadata": {},
   "source": [
    "## 12. Organisation vs. datasource totals comparison\n",
    "Compare each organisation's total research products with the combined totals of all datasources linked to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9779988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = DATA_DIR / \"nl_orgs_openaire_datasources_numFound_history.xlsx\"\n",
    "datasources_path = DATA_DIR / \"nl_orgs_openaire_datasources.xlsx\"\n",
    "orgs_path = DATA_DIR / \"nl_orgs_openaire.xlsx\"\n",
    "if not (history_path.exists() and datasources_path.exists() and orgs_path.exists()):\n",
    "    print(\"One or more required files are missing. Please run Steps 3, 7, and 8 before this comparison.\")\n",
    "else:\n",
    "    history_df = pd.read_excel(history_path)\n",
    "    datasources_df = pd.read_excel(datasources_path)\n",
    "    orgs_df = pd.read_excel(orgs_path)\n",
    "\n",
    "    numeric_columns = [\n",
    "        \"Total Research Products\",\n",
    "        *PRODUCT_TYPE_LABELS.values(),\n",
    "    ]\n",
    "    for column in numeric_columns:\n",
    "        if column in history_df.columns:\n",
    "            history_df[column] = pd.to_numeric(history_df[column], errors=\"coerce\")\n",
    "        if column in orgs_df.columns:\n",
    "            orgs_df[column] = pd.to_numeric(orgs_df[column], errors=\"coerce\")\n",
    "\n",
    "    history_df[\"date_retrieved\"] = pd.to_datetime(history_df[\"date_retrieved\"], errors=\"coerce\")\n",
    "    latest_date = history_df[\"date_retrieved\"].max()\n",
    "    datasource_latest = history_df[history_df[\"date_retrieved\"] == latest_date].copy()\n",
    "\n",
    "    if datasource_latest.empty:\n",
    "        print(\"No datasource snapshot found for comparison.\")\n",
    "    else:\n",
    "        datasource_latest[\"Total Research Products\"] = datasource_latest[\"Total Research Products\"].fillna(0)\n",
    "        link_df = datasources_df[[\"OpenAIRE_DataSource_ID\", \"OpenAIRE_ORG_ID\"]].copy()\n",
    "        mapped = link_df.merge(\n",
    "            datasource_latest[[\"OpenAIRE_DataSource_ID\", \"Total Research Products\"]],\n",
    "            on=\"OpenAIRE_DataSource_ID\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        mapped[\"Total Research Products\"] = mapped[\"Total Research Products\"].fillna(0)\n",
    "        datasource_by_org = mapped.groupby(\"OpenAIRE_ORG_ID\")[\"Total Research Products\"].sum()\n",
    "\n",
    "        org_totals = orgs_df[[\"name\", \"OpenAIRE_ORG_ID\", \"Total Research Products\"]].copy()\n",
    "        org_totals[\"Total Research Products\"] = org_totals[\"Total Research Products\"].fillna(0)\n",
    "        org_totals[\"Datasource totals\"] = org_totals[\"OpenAIRE_ORG_ID\"].map(datasource_by_org).fillna(0)\n",
    "\n",
    "        if org_totals.empty:\n",
    "            print(\"Organisation totals not available for comparison.\")\n",
    "        else:\n",
    "            org_totals = org_totals.sort_values(\"Total Research Products\", ascending=False)\n",
    "            max_label_length = 40\n",
    "            display_names = org_totals[\"name\"].astype(str).apply(\n",
    "                lambda text: text if len(text) <= max_label_length else text[: max_label_length - 1] + \"…\"\n",
    "            )\n",
    "            indices = range(len(org_totals))\n",
    "            bar_height = 0.4\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(12, 20))\n",
    "            ax.barh(\n",
    "                [i + bar_height / 2 for i in indices],\n",
    "                org_totals[\"Total Research Products\"],\n",
    "                height=bar_height,\n",
    "                color=\"#1f77b4\",\n",
    "                label=\"Organisation total\",\n",
    "            )\n",
    "            ax.barh(\n",
    "                [i - bar_height / 2 for i in indices],\n",
    "                org_totals[\"Datasource totals\"],\n",
    "                height=bar_height,\n",
    "                color=\"#2ca02c\",\n",
    "                label=\"Combined datasource totals\",\n",
    "            )\n",
    "\n",
    "            ax.set_yticks(list(indices))\n",
    "            ax.set_yticklabels(display_names)\n",
    "            ax.set_xlabel(\"Total research products\")\n",
    "            ax.set_ylabel(\"Organisation\")\n",
    "            ax.set_title(\n",
    "                f\"Organisation vs. datasource totals (snapshot {latest_date.date()})\"\n",
    "            )\n",
    "            ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.08), ncol=2)\n",
    "            ax.invert_yaxis()\n",
    "            fig.tight_layout()\n",
    "            org_vs_ds_path = IMG_DIR / \"org_vs_datasources.png\"\n",
    "            fig.savefig(org_vs_ds_path, dpi=200, bbox_inches=\"tight\")\n",
    "            print(f\"Saved organisation vs datasource comparison chart to {org_vs_ds_path}\")\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa59974",
   "metadata": {},
   "source": [
    "## 13. Organisation vs. individual datasource breakdown\n",
    "Display each organisation alongside its own datasources for the latest snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb407d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = DATA_DIR / \"nl_orgs_openaire_datasources_numFound_history.xlsx\"\n",
    "datasources_path = DATA_DIR / \"nl_orgs_openaire_datasources.xlsx\"\n",
    "orgs_path = DATA_DIR / \"nl_orgs_openaire.xlsx\"\n",
    "if not (history_path.exists() and datasources_path.exists() and orgs_path.exists()):\n",
    "    print(\"One or more required files are missing. Run Steps 3, 7, and 8 first.\")\n",
    "else:\n",
    "    history_df = pd.read_excel(history_path)\n",
    "    datasources_df = pd.read_excel(datasources_path)\n",
    "    orgs_df = pd.read_excel(orgs_path)\n",
    "\n",
    "    numeric_columns = [\n",
    "        \"Total Research Products\",\n",
    "        *PRODUCT_TYPE_LABELS.values(),\n",
    "    ]\n",
    "    for column in numeric_columns:\n",
    "        if column in history_df.columns:\n",
    "            history_df[column] = pd.to_numeric(history_df[column], errors=\"coerce\")\n",
    "        if column in orgs_df.columns:\n",
    "            orgs_df[column] = pd.to_numeric(orgs_df[column], errors=\"coerce\")\n",
    "\n",
    "    history_df[\"date_retrieved\"] = pd.to_datetime(history_df[\"date_retrieved\"], errors=\"coerce\")\n",
    "    latest_date = history_df[\"date_retrieved\"].max()\n",
    "    datasource_latest = history_df[history_df[\"date_retrieved\"] == latest_date].copy()\n",
    "\n",
    "    if datasource_latest.empty:\n",
    "        print(\"No datasource snapshot found for breakdown chart.\")\n",
    "    else:\n",
    "        datasource_latest[\"Total Research Products\"] = datasource_latest[\"Total Research Products\"].fillna(0)\n",
    "        ds_with_org = datasources_df[[\n",
    "            \"OpenAIRE_DataSource_ID\",\n",
    "            \"OpenAIRE_ORG_ID\",\n",
    "            \"Name\",\n",
    "        ]].merge(\n",
    "            datasource_latest[[\"OpenAIRE_DataSource_ID\", \"Total Research Products\"]],\n",
    "            on=\"OpenAIRE_DataSource_ID\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        ds_with_org[\"Total Research Products\"] = ds_with_org[\"Total Research Products\"].fillna(0)\n",
    "        orgs_df[\"Total Research Products\"] = orgs_df[\"Total Research Products\"].fillna(0)\n",
    "\n",
    "        records = []\n",
    "        zero_datasources: list[str] = []\n",
    "\n",
    "        for _, org_row in orgs_df.sort_values(\"Total Research Products\", ascending=False).iterrows():\n",
    "            org_name = org_row.get(\"name\", \"Unknown organisation\")\n",
    "            org_id = org_row.get(\"OpenAIRE_ORG_ID\")\n",
    "            org_value = float(org_row.get(\"Total Research Products\") or 0)\n",
    "            records.append({\"label\": org_name, \"value\": org_value, \"color\": \"#1f77b4\"})\n",
    "\n",
    "            org_ds = ds_with_org[ds_with_org[\"OpenAIRE_ORG_ID\"] == org_id]\n",
    "            if org_ds.empty:\n",
    "                continue\n",
    "\n",
    "            for _, ds_row in org_ds.sort_values(\"Total Research Products\", ascending=False).iterrows():\n",
    "                ds_value = float(ds_row.get(\"Total Research Products\") or 0)\n",
    "                ds_name = ds_row.get(\"Name\") or ds_row.get(\"OpenAIRE_DataSource_ID\")\n",
    "                if ds_value <= 0:\n",
    "                    zero_datasources.append(f\"{ds_name} (org: {org_name})\")\n",
    "                    continue\n",
    "                label = f\"  ↳ {ds_name}\"\n",
    "                records.append({\"label\": label, \"value\": ds_value, \"color\": \"#2ca02c\"})\n",
    "\n",
    "        if zero_datasources:\n",
    "            print(\"Datasources with zero totals (excluded):\")\n",
    "            for entry in zero_datasources:\n",
    "                print(f\" - {entry}\")\n",
    "\n",
    "        if not records:\n",
    "            print(\"No datapoints available for the breakdown chart.\")\n",
    "    \n",
    "        else:\n",
    "            values = [rec[\"value\"] for rec in records]\n",
    "            labels = [rec[\"label\"] for rec in records]\n",
    "            colors = [rec[\"color\"] for rec in records]\n",
    "            fig_height = max(6, 0.4 * len(records))\n",
    "            fig, ax = plt.subplots(figsize=(14, fig_height))\n",
    "            ax.barh(range(len(records)), values, color=colors)\n",
    "            ax.set_yticks(range(len(records)))\n",
    "            ax.set_yticklabels(labels)\n",
    "            ax.set_xlabel(\"Total research products\")\n",
    "            ax.set_ylabel(\"Organisation / Datasource\")\n",
    "            ax.set_title(\n",
    "                f\"Organisation vs. individual datasource totals (snapshot {latest_date.date()})\"\n",
    "            )\n",
    "            ax.invert_yaxis()\n",
    "            fig.tight_layout()\n",
    "            breakdown_path = IMG_DIR / \"org_vs_datasource_breakdown.png\"\n",
    "            fig.savefig(breakdown_path, dpi=200, bbox_inches=\"tight\")\n",
    "            print(f\"Saved organisation/datasource breakdown chart to {breakdown_path}\")\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f84dd7a",
   "metadata": {},
   "source": [
    "## 14. Download curated OAI endpoint list\n",
    "Fetch the curated OAI-PMH spreadsheet and store it locally so downstream steps can reference the latest endpoint information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592128bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CURATED_OAI_URL = (\n",
    "    \"https://docs.google.com/spreadsheets/d/e/\"\n",
    "    \"2PACX-1vQwM24DIUWmqbjxaAy62w9w8gNpOMSg5sxmFro-OexCeMzIlyUJh5iVVsVxyrcLkQ/pub?output=xlsx\"\n",
    ")\n",
    "curated_path = DATA_DIR / \"curated_oai_endpoints.xlsx\"\n",
    "\n",
    "response = requests.get(CURATED_OAI_URL, timeout=30)\n",
    "response.raise_for_status()\n",
    "curated_path.write_bytes(response.content)\n",
    "print(f\"Saved curated OAI endpoints to {curated_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5bb27c",
   "metadata": {},
   "source": [
    "## 15. Merge curated endpoints into datasource export\n",
    "Use the curated sheet to backfill `OAI-endpoint` values in the datasource workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b44117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ast\n",
    "\n",
    "curated_path = DATA_DIR / \"curated_oai_endpoints.xlsx\"\n",
    "datasources_path = DATA_DIR / \"nl_orgs_openaire_datasources.xlsx\"\n",
    "output_path = DATA_DIR / \"nl_orgs_openaire_datasources_with_endpoint.xlsx\"\n",
    "\n",
    "if not curated_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing curated endpoint workbook: {curated_path}. Run step 14 first.\"\n",
    "    )\n",
    "if not datasources_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing datasource export: {datasources_path}. Run step 7 first.\"\n",
    "    )\n",
    "\n",
    "datasources_df = pd.read_excel(datasources_path)\n",
    "curated_df = pd.read_excel(curated_path)\n",
    "\n",
    "def normalize_id(value):\n",
    "    return str(value).strip() if pd.notna(value) else None\n",
    "\n",
    "def clean_endpoint(value):\n",
    "    if not isinstance(value, str):\n",
    "        return None\n",
    "    text = value.strip()\n",
    "    if not text:\n",
    "        return None\n",
    "    if text.startswith(\"[\") and text.endswith(\"]\"):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(text)\n",
    "            if isinstance(parsed, (list, tuple)) and parsed:\n",
    "                text = \"\" if parsed[0] is None else str(parsed[0])\n",
    "            elif isinstance(parsed, str):\n",
    "                text = parsed\n",
    "            else:\n",
    "                text = \"\"\n",
    "        except Exception:\n",
    "            text = text.strip(\"[]\")\n",
    "    text = text.strip().strip(\"\\\"'[]\")\n",
    "    if not text:\n",
    "        return None\n",
    "    if text.startswith(\"//\"):\n",
    "        text = f\"https:{text}\"\n",
    "    elif not text.startswith((\"http://\", \"https://\")):\n",
    "        text = f\"https://{text}\"\n",
    "    return text\n",
    "\n",
    "id_col = \"OpenAIRE_DataSource_ID\"\n",
    "if id_col not in curated_df.columns:\n",
    "    raise ValueError(\"Curated workbook missing OpenAIRE_DataSource_ID column\")\n",
    "\n",
    "endpoint_col = None\n",
    "for candidate in [\"oai_endpoint\", \"OAI_endpoint\", \"endpoint\", \"OAI-endpoint\"]:\n",
    "    if candidate in curated_df.columns:\n",
    "        endpoint_col = candidate\n",
    "        break\n",
    "if endpoint_col is None:\n",
    "    raise ValueError(\"Curated workbook missing an oai_endpoint column\")\n",
    "\n",
    "datasources_df[id_col] = datasources_df[id_col].apply(normalize_id)\n",
    "curated_df[id_col] = curated_df[id_col].apply(normalize_id)\n",
    "curated_df[endpoint_col] = curated_df[endpoint_col].apply(clean_endpoint)\n",
    "\n",
    "endpoint_map = (\n",
    "    curated_df.dropna(subset=[id_col, endpoint_col])\n",
    "              .drop_duplicates(subset=[id_col], keep=\"first\")\n",
    "              .set_index(id_col)[endpoint_col]\n",
    "              .to_dict()\n",
    ")\n",
    "\n",
    "ensure_object = lambda s: s.astype(\"object\") if s.dtype != \"object\" else s\n",
    "if \"OAI-endpoint\" not in datasources_df.columns:\n",
    "    datasources_df[\"OAI-endpoint\"] = pd.Series(pd.NA, index=datasources_df.index, dtype=\"object\")\n",
    "datasources_df[\"OAI-endpoint\"] = ensure_object(datasources_df[\"OAI-endpoint\"])\n",
    "\n",
    "mapped = datasources_df[id_col].map(endpoint_map)\n",
    "updated_mask = mapped.notna()\n",
    "datasources_df.loc[updated_mask, \"OAI-endpoint\"] = mapped[updated_mask]\n",
    "\n",
    "debug_examples = datasources_df.loc[updated_mask, [id_col, \"OAI-endpoint\"]].head(10)\n",
    "print(\"Sample endpoint mappings:\")\n",
    "if debug_examples.empty:\n",
    "    print(\"  No endpoints were updated.\")\n",
    "else:\n",
    "    print(debug_examples.to_string(index=False))\n",
    "\n",
    "datasources_df.to_excel(output_path, index=False)\n",
    "print(\n",
    "    f\"Updated {updated_mask.sum()} datasource endpoints using curated sheet; saved to {output_path}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5dd041",
   "metadata": {},
   "source": [
    "## 16. Probe OAI-PMH endpoints and capture supported formats\n",
    "Validate every datasource OAI endpoint against the OAI-PMH specification and OpenAIRE/Dutch profile requirements, recording which mandatory metadata prefixes are actually exposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36122fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "datasources_with_endpoint_path = DATA_DIR / \"nl_orgs_openaire_datasources_with_endpoint.xlsx\"\n",
    "metrics_output_path = DATA_DIR / \"nl_orgs_openaire_datasources_with_endpoint_metrics.xlsx\"\n",
    "\n",
    "if not datasources_with_endpoint_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing endpoint-enriched datasources file: {datasources_with_endpoint_path}. Run step 15 first.\"\n",
    "    )\n",
    "\n",
    "metrics_df = pd.read_excel(datasources_with_endpoint_path)\n",
    "\n",
    "if \"OpenAIRE_DataSource_ID\" not in metrics_df.columns:\n",
    "    raise ValueError(\"Input file missing OpenAIRE_DataSource_ID column.\")\n",
    "\n",
    "detection_columns = {\n",
    "    \"detected_support_nl_didl\": {\"nl_didl\"},\n",
    "    \"detected_support_oai_dc\": {\"oai_dc\"},\n",
    "    \"detected_support_oai_openaire\": {\"oai_openaire\"},\n",
    "    \"detected_support_rioxx\": {\"rioxx\", \"rioxxv2\"},\n",
    "    \"detected_support_oai_cerif_openaire\": {\"cerif_openaire\", \"oai_cerif_openaire\"},\n",
    "    \"detected_support_openaire_data\": {\"oai_datacite\", \"datacite\", \"oai_openaire_data\", \"openaire_data\"},\n",
    "}\n",
    "\n",
    "for column in detection_columns.keys():\n",
    "    metrics_df[column] = False\n",
    "\n",
    "metrics_df[\"oai_status\"] = \"not_tested\"\n",
    "metrics_df[\"oai_error\"] = pd.NA\n",
    "metrics_df[\"metadata_prefixes_detected\"] = pd.NA\n",
    "metrics_df[\"oai_tested_at_utc\"] = pd.NA\n",
    "\n",
    "def normalise_endpoint(url: str) -> list[str]:\n",
    "    if not isinstance(url, str):\n",
    "        return []\n",
    "    raw = url.strip()\n",
    "    if not raw:\n",
    "        return []\n",
    "    candidates = []\n",
    "    parsed = urlparse(raw)\n",
    "    if parsed.scheme:\n",
    "        candidates.append(raw)\n",
    "        if parsed.scheme == \"http\":\n",
    "            candidates.append(raw.replace(\"http://\", \"https://\", 1))\n",
    "    else:\n",
    "        candidates.append(f\"https://{raw}\")\n",
    "        candidates.append(f\"http://{raw}\")\n",
    "    seen: list[str] = []\n",
    "    for cand in candidates:\n",
    "        if cand not in seen:\n",
    "            seen.append(cand)\n",
    "    return seen\n",
    "\n",
    "def build_oai_url(base: str, verb: str = \"ListMetadataFormats\") -> str:\n",
    "    if base.endswith(\"?\") or base.endswith(\"&\"):\n",
    "        return f\"{base}verb={verb}\"\n",
    "    return f\"{base}&verb={verb}\" if \"?\" in base else f\"{base}?verb={verb}\"\n",
    "\n",
    "def parse_metadata_formats(xml_bytes: bytes) -> tuple[list[str], str | None]:\n",
    "    try:\n",
    "        root = ET.fromstring(xml_bytes)\n",
    "    except ET.ParseError as exc:\n",
    "        return [], f\"XML parse error: {exc}\"\n",
    "    ns = {\"oai\": \"http://www.openarchives.org/OAI/2.0/\"}\n",
    "    prefixes = [\n",
    "        (el.text or \"\").strip().lower()\n",
    "        for el in root.findall(\".//oai:metadataPrefix\", namespaces=ns)\n",
    "        if (el.text or \"\").strip()\n",
    "    ]\n",
    "    if prefixes:\n",
    "        return prefixes, None\n",
    "    errors = root.findall(\".//oai:error\", namespaces=ns)\n",
    "    if errors:\n",
    "        messages = [\n",
    "            f\"{err.get('code', 'error')}: {(err.text or '').strip()}\" for err in errors\n",
    "        ]\n",
    "        return [], \"; \".join(messages)\n",
    "    return [], \"No metadataPrefix elements returned\"\n",
    "\n",
    "def test_endpoint(endpoint: str) -> dict[str, Any]:\n",
    "    result = {\n",
    "        \"oai_status\": \"missing_endpoint\" if not endpoint else \"error\",\n",
    "        \"oai_error\": None,\n",
    "        \"metadata_prefixes_detected\": None,\n",
    "    }\n",
    "    prefix_flags = {column: False for column in detection_columns.keys()}\n",
    "    if not endpoint:\n",
    "        result.update(prefix_flags)\n",
    "        return result\n",
    "\n",
    "    prefixes: list[str] = []\n",
    "    errors: list[str] = []\n",
    "    for candidate in normalise_endpoint(endpoint):\n",
    "        url = build_oai_url(candidate)\n",
    "        try:\n",
    "            resp = requests.get(url, timeout=25, headers={\"User-Agent\": API_USER_AGENT})\n",
    "            resp.raise_for_status()\n",
    "        except Exception as exc:\n",
    "            errors.append(f\"{candidate}: {exc}\")\n",
    "            continue\n",
    "        prefixes, error = parse_metadata_formats(resp.content)\n",
    "        if prefixes:\n",
    "            result[\"oai_status\"] = \"ok\"\n",
    "            errors = []\n",
    "            break\n",
    "        if error:\n",
    "            errors.append(f\"{candidate}: {error}\")\n",
    "\n",
    "    if prefixes:\n",
    "        detected_str = \", \".join(sorted(set(prefixes)))\n",
    "        result[\"metadata_prefixes_detected\"] = detected_str\n",
    "        for column, required in detection_columns.items():\n",
    "            prefix_flags[column] = any(prefix in required for prefix in prefixes)\n",
    "    else:\n",
    "        result[\"oai_error\"] = \"; \".join(errors) if errors else \"Unknown error\"\n",
    "\n",
    "    result.update(prefix_flags)\n",
    "    return result\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "    futures = {\n",
    "        executor.submit(test_endpoint, row.get(\"OAI-endpoint\")): idx\n",
    "        for idx, row in metrics_df.iterrows()\n",
    "    }\n",
    "    for future in tqdm(\n",
    "        as_completed(futures),\n",
    "        total=len(futures),\n",
    "        desc=\"Testing OAI endpoints\",\n",
    "        unit=\"datasource\",\n",
    "    ):\n",
    "        idx = futures[future]\n",
    "        outcome = future.result()\n",
    "        for key, value in outcome.items():\n",
    "            metrics_df.at[idx, key] = value\n",
    "        metrics_df.at[idx, \"oai_tested_at_utc\"] = pd.Timestamp.utcnow().isoformat()\n",
    "\n",
    "metrics_df.to_excel(metrics_output_path, index=False)\n",
    "print(\n",
    "    f\"Saved OAI endpoint diagnostics for {len(metrics_df)} datasources to {metrics_output_path}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a7646d",
   "metadata": {},
   "source": [
    "## 17. Visualise OAI endpoint coverage\n",
    "Summaries of endpoint availability, test outcomes, and required metadata-format support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d542297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics_path = DATA_DIR / \"nl_orgs_openaire_datasources_with_endpoint_metrics.xlsx\"\n",
    "if not metrics_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing OAI diagnostics: {metrics_path}. Run step 16 first.\"\n",
    "    )\n",
    "\n",
    "metrics_df = pd.read_excel(metrics_path)\n",
    "\n",
    "def is_blank(series: pd.Series) -> pd.Series:\n",
    "    text = series.astype(str).str.strip().str.lower()\n",
    "    blank_tokens = {\"\", \"nan\", \"<na>\", \"none\", \"null\"}\n",
    "    return series.isna() | text.isin(blank_tokens)\n",
    "\n",
    "total_endpoints = (~is_blank(metrics_df[\"OAI-endpoint\"])).sum()\n",
    "passed = (metrics_df[\"oai_status\"] == \"ok\").sum()\n",
    "failed = (metrics_df[\"oai_status\"] == \"error\").sum()\n",
    "\n",
    "def is_indexed(value) -> bool:\n",
    "    if not isinstance(value, str):\n",
    "        return False\n",
    "    text = value.strip().lower()\n",
    "    if not text or \"not yet\" in text:\n",
    "        return False\n",
    "    keywords = [\"openaire\", \"collected from a compatible aggregator\"]\n",
    "    return any(keyword in text for keyword in keywords)\n",
    "\n",
    "indexed = metrics_df[\"openaireCompatibility\"].apply(is_indexed).sum()\n",
    "\n",
    "type_series = metrics_df[\"Type\"].fillna(\"\")\n",
    "cris_mask = type_series.str.contains(\"cris\", case=False, na=False)\n",
    "literature_mask = type_series.str.contains(\"literature|institutional\", case=False, na=False)\n",
    "data_mask = type_series.str.contains(\"data repository\", case=False, na=False)\n",
    "\n",
    "cris_cerif = (\n",
    "    cris_mask & metrics_df.get(\"detected_support_oai_cerif_openaire\", False).astype(bool)\n",
    ").sum()\n",
    "literature_openaire = (\n",
    "    literature_mask & metrics_df.get(\"detected_support_oai_openaire\", False).astype(bool)\n",
    ").sum()\n",
    "data_openaire = (\n",
    "    data_mask & metrics_df.get(\"detected_support_openaire_data\", False).astype(bool)\n",
    ").sum()\n",
    "\n",
    "nl_didl = metrics_df.get(\"detected_support_nl_didl\", False).astype(bool).sum()\n",
    "rioxx = metrics_df.get(\"detected_support_rioxx\", False).astype(bool).sum()\n",
    "\n",
    "summary = pd.DataFrame(\n",
    "    {\n",
    "        \"metric\": [\n",
    "            \"Endpoints available\",\n",
    "            \"OAI test passed\",\n",
    "            \"OAI test failed\",\n",
    "            \"Indexed by OpenAIRE\",\n",
    "            \"CRIS endpoints w/ CERIF\",\n",
    "            \"Literature endpoints w/ OpenAIRE\",\n",
    "            \"Data endpoints w/ OpenAIRE data\",\n",
    "            \"Endpoints w/ NL DIDL\",\n",
    "            \"Endpoints w/ RIOXX\",\n",
    "        ],\n",
    "        \"count\": [\n",
    "            total_endpoints,\n",
    "            passed,\n",
    "            failed,\n",
    "            indexed,\n",
    "            cris_cerif,\n",
    "            literature_openaire,\n",
    "            data_openaire,\n",
    "            nl_didl,\n",
    "            rioxx,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "display(summary)\n",
    "\n",
    "summary_chart_path = IMG_DIR / \"oai_endpoint_summary.png\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.barh(summary[\"metric\"], summary[\"count\"], color=\"#2d7fb8\")\n",
    "ax.set_xlabel(\"Count\")\n",
    "ax.set_title(\"OAI endpoint coverage and compliance\")\n",
    "ax.invert_yaxis()\n",
    "for i, value in enumerate(summary[\"count\"]):\n",
    "    ax.text(value + max(summary[\"count\"]) * 0.01, i, f\"{value}\", va=\"center\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(summary_chart_path, dpi=150)\n",
    "plt.show()\n",
    "\n",
    "compat_df = metrics_df.copy()\n",
    "compat_df[\"Type\"] = compat_df[\"Type\"].fillna(\"Unknown\")\n",
    "compat_df[\"openaireCompatibility\"] = compat_df[\"openaireCompatibility\"].fillna(\"Unknown\")\n",
    "\n",
    "compat_counts = (\n",
    "    compat_df.groupby([\"Type\", \"openaireCompatibility\"], dropna=False)\n",
    "             .size()\n",
    "             .reset_index(name=\"count\")\n",
    ")\n",
    "compat_pivot = compat_counts.pivot(\n",
    "    index=\"Type\", columns=\"openaireCompatibility\", values=\"count\"\n",
    ").fillna(0)\n",
    "compat_pivot = compat_pivot.sort_values(by=compat_pivot.columns.tolist(), ascending=False)\n",
    "\n",
    "compat_chart_path = IMG_DIR / \"oai_openaire_compatibility_by_type.png\"\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(12, max(4, 0.4 * len(compat_pivot))))\n",
    "compat_pivot.plot(kind=\"barh\", stacked=True, ax=ax2, colormap=\"tab20\")\n",
    "ax2.set_xlabel(\"Datasources\")\n",
    "ax2.set_ylabel(\"Type\")\n",
    "ax2.set_title(\"OpenAIRE compatibility by datasource type\")\n",
    "ax2.legend(title=\"openaireCompatibility\", bbox_to_anchor=(1.0, 1.0))\n",
    "plt.tight_layout()\n",
    "plt.savefig(compat_chart_path, dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"Saved charts to {summary_chart_path} and {compat_chart_path}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0017ddd6",
   "metadata": {},
   "source": [
    "## 18. Interactive datasource dashboard\n",
    "Filter by organisation to review datasource health, latest volumes, and metadata compliance in a single view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b89c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading organisations from data/nl_orgs_openaire.xlsx ...\n",
      "Loaded 93 organisation rows\n",
      "Loading datasource history from data/nl_orgs_openaire_datasources_numFound_history.xlsx ...\n",
      "Loaded history with 170 rows\n",
      "Loading endpoint metrics from data/nl_orgs_openaire_datasources_with_endpoint_metrics.xlsx ...\n",
      "Loaded metrics with 170 rows\n",
      "Latest snapshot date detected: 2025-11-07\n",
      "Latest snapshot records: 170\n",
      "Merging metrics with snapshot and organisations ...\n",
      "Dashboard dataframe size: (224, 31)\n",
      "Dashboard organisations available: 46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a4945b2a9b4242af50997bad19496c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Organisation', options=('All organisations', 'AMOLF, Physics of functional complex matte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dabac1f47069424cb38e4a2012b0a13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering dashboard for 'All organisations' with 224 datasource rows\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "OPENAIRE_ORG_URL = \"https://explore.openaire.eu/search/organization?organizationId=\"\n",
    "OPENAIRE_DATASOURCE_URL = \"https://explore.openaire.eu/search/dataprovider?datasourceId=\"\n",
    "OAI_IDENTIFY_PARAM = \"verb=Identify\"\n",
    "\n",
    "orgs_path = DATA_DIR / \"nl_orgs_openaire.xlsx\"\n",
    "history_path = DATA_DIR / \"nl_orgs_openaire_datasources_numFound_history.xlsx\"\n",
    "metrics_path = DATA_DIR / \"nl_orgs_openaire_datasources_with_endpoint_metrics.xlsx\"\n",
    "\n",
    "print(f\"Loading organisations from {orgs_path} ...\")\n",
    "orgs_df = pd.read_excel(orgs_path)[[\"OpenAIRE_ORG_ID\", \"name\"]].rename(columns={\"name\": \"Organisation\"})\n",
    "print(f\"Loaded {len(orgs_df)} organisation rows\")\n",
    "\n",
    "print(f\"Loading datasource history from {history_path} ...\")\n",
    "history_df = pd.read_excel(history_path)\n",
    "print(f\"Loaded history with {len(history_df)} rows\")\n",
    "\n",
    "print(f\"Loading endpoint metrics from {metrics_path} ...\")\n",
    "metrics_df = pd.read_excel(metrics_path)\n",
    "print(f\"Loaded metrics with {len(metrics_df)} rows\")\n",
    "\n",
    "if history_df.empty:\n",
    "    raise ValueError(\"History workbook is empty; run the snapshot steps first.\")\n",
    "\n",
    "latest_date = history_df[\"date_retrieved\"].max()\n",
    "print(f\"Latest snapshot date detected: {latest_date}\")\n",
    "latest_snapshot = history_df[history_df[\"date_retrieved\"] == latest_date].copy()\n",
    "print(f\"Latest snapshot records: {len(latest_snapshot)}\")\n",
    "\n",
    "value_cols = [\n",
    "    \"Total Research Products\",\n",
    "    \"Publications\",\n",
    "    \"Research data\",\n",
    "    \"Research software\",\n",
    "    \"Other research products\",\n",
    "]\n",
    "\n",
    "latest_snapshot = latest_snapshot[[\"OpenAIRE_DataSource_ID\", *value_cols, \"date_retrieved\"]]\n",
    "\n",
    "print(\"Merging metrics with snapshot and organisations ...\")\n",
    "dashboard_df = (\n",
    "    metrics_df.merge(latest_snapshot, on=\"OpenAIRE_DataSource_ID\", how=\"left\", suffixes=(\"\", \"_latest\"))\n",
    "              .merge(orgs_df, on=\"OpenAIRE_ORG_ID\", how=\"left\")\n",
    ")\n",
    "print(f\"Dashboard dataframe size: {dashboard_df.shape}\")\n",
    "\n",
    "bool_cols = [\n",
    "    \"detected_support_oai_cerif_openaire\",\n",
    "    \"detected_support_oai_openaire\",\n",
    "    \"detected_support_openaire_data\",\n",
    "    \"detected_support_nl_didl\",\n",
    "    \"detected_support_rioxx\",\n",
    "]\n",
    "\n",
    "dashboard_df[bool_cols] = dashboard_df[bool_cols].fillna(False)\n",
    "dashboard_df[\"Organisation Name\"] = dashboard_df[\"Organisation\"].fillna(\"Unknown organisation\")\n",
    "dashboard_df[\"Datasource Name\"] = dashboard_df[\"Name\"].fillna(\"Unknown datasource\")\n",
    "dashboard_df[\"OpenAIRE Compatibility\"] = dashboard_df[\"openaireCompatibility\"].fillna(\"Not specified\")\n",
    "dashboard_df[\"has_endpoint\"] = dashboard_df[\"OAI-endpoint\"].astype(str).str.strip().ne(\"\")\n",
    "\n",
    "def build_metric_number(value: int | str) -> str:\n",
    "    if isinstance(value, str):\n",
    "        return html.escape(value)\n",
    "    return f\"{int(value):,}\"\n",
    "\n",
    "SUMMARY_STYLE = \"\"\"\n",
    "<style>\n",
    ".metric-row {\n",
    "  display: flex;\n",
    "  flex-wrap: wrap;\n",
    "  gap: 1rem;\n",
    "  margin-bottom: 1rem;\n",
    "}\n",
    ".metric-card {\n",
    "  padding: 0.75rem 1rem;\n",
    "  border: 1px solid #d9d9d9;\n",
    "  border-radius: 0.5rem;\n",
    "  min-width: 10rem;\n",
    "  flex: 1;\n",
    "}\n",
    ".metric-value {\n",
    "  font-size: 1.5rem;\n",
    "  font-weight: bold;\n",
    "}\n",
    ".metric-label {\n",
    "  font-size: 0.85rem;\n",
    "  color: #555;\n",
    "  margin-top: 0.25rem;\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "org_options = [\"All organisations\"] + sorted(\n",
    "    dashboard_df[\"Organisation\"].dropna().unique().tolist()\n",
    ")\n",
    "\n",
    "dropdown = widgets.Dropdown(options=org_options, description=\"Organisation\", value=\"All organisations\")\n",
    "datasource_dropdown = widgets.Dropdown(options=[(\"All datasources\", \"__all__\")], description=\"Datasource\", value=\"__all__\")\n",
    "summary_widget = widgets.HTML(value=\"\")\n",
    "output = widgets.Output()\n",
    "\n",
    "suppress_datasource_event = False\n",
    "\n",
    "def update_datasource_options():\n",
    "    global suppress_datasource_event\n",
    "    selection = dropdown.value\n",
    "    base = dashboard_df if selection == \"All organisations\" else dashboard_df[dashboard_df[\"Organisation\"] == selection]\n",
    "    ds_subset = base.dropna(subset=[\"OpenAIRE_DataSource_ID\"]).drop_duplicates(\"OpenAIRE_DataSource_ID\")\n",
    "    ds_subset = ds_subset.sort_values(\"Datasource Name\")\n",
    "    options = [(\"All datasources\", \"__all__\")]\n",
    "    for _, row in ds_subset.iterrows():\n",
    "        options.append((row[\"Datasource Name\"], row[\"OpenAIRE_DataSource_ID\"]))\n",
    "    valid_values = {value for _, value in options}\n",
    "    suppress_datasource_event = True\n",
    "    datasource_dropdown.options = options\n",
    "    if datasource_dropdown.value not in valid_values:\n",
    "        datasource_dropdown.value = \"__all__\"\n",
    "    suppress_datasource_event = False\n",
    "\n",
    "def format_summary(selection_df: pd.DataFrame) -> str:\n",
    "    org_count = selection_df[\"OpenAIRE_ORG_ID\"].nunique(dropna=True)\n",
    "    datasource_count = selection_df[\"OpenAIRE_DataSource_ID\"].nunique(dropna=True)\n",
    "    compat_mask = selection_df[\"OpenAIRE Compatibility\"].str.lower().ne(\"not specified\")\n",
    "    compat_count = selection_df.loc[compat_mask, \"OpenAIRE_DataSource_ID\"].nunique(dropna=True)\n",
    "    active_endpoints = selection_df.loc[selection_df[\"oai_status\"].fillna(\"\").str.lower() == \"ok\", \"OpenAIRE_DataSource_ID\"].nunique(dropna=True)\n",
    "    nl_didl = int(selection_df[\"detected_support_nl_didl\"].sum())\n",
    "    cerif = int(selection_df[\"detected_support_oai_cerif_openaire\"].sum())\n",
    "    literature = int(selection_df[\"detected_support_oai_openaire\"].sum())\n",
    "    data_support = int(selection_df[\"detected_support_openaire_data\"].sum())\n",
    "    date_label = pd.to_datetime(latest_date).strftime(\"%Y-%m-%d\") if pd.notna(latest_date) else \"Unknown\"\n",
    "\n",
    "    stats = [\n",
    "        (\"Dutch research organisations\", build_metric_number(org_count)),\n",
    "        (\"Dutch data sources\", build_metric_number(datasource_count)),\n",
    "        (\"OpenAIRE compatible data sources\", build_metric_number(compat_count)),\n",
    "        (\"Active OAI endpoints\", build_metric_number(active_endpoints)),\n",
    "        (\"NL DIDL supported\", build_metric_number(nl_didl)),\n",
    "        (\"OpenAIRE CERIF supported\", build_metric_number(cerif)),\n",
    "        (\"OpenAIRE literature supported\", build_metric_number(literature)),\n",
    "        (\"OpenAIRE data supported\", build_metric_number(data_support)),\n",
    "        (\"Latest snapshot date\", build_metric_number(date_label)),\n",
    "    ]\n",
    "\n",
    "    cards = []\n",
    "    for label, value in stats:\n",
    "        cards.append(\n",
    "            f\"<div class='metric-card'><div class='metric-value'>{value}</div><div class='metric-label'>{html.escape(label)}</div></div>\"\n",
    "        )\n",
    "    return SUMMARY_STYLE + \"<div class='metric-row'>\" + \"\".join(cards) + \"</div>\"\n",
    "\n",
    "def append_identify_param(url) -> str:\n",
    "    if pd.isna(url):\n",
    "        return \"\"\n",
    "    clean = str(url).strip()\n",
    "    if not clean:\n",
    "        return \"\"\n",
    "    if \"verb=Identify\" in clean:\n",
    "        return clean\n",
    "    return f\"{clean}{'&' if '?' in clean else '?'}{OAI_IDENTIFY_PARAM}\"\n",
    "\n",
    "def render_dashboard(change=None):\n",
    "    selection = dropdown.value\n",
    "    datasource_selection = datasource_dropdown.value\n",
    "    filtered = dashboard_df if selection == \"All organisations\" else dashboard_df[dashboard_df[\"Organisation\"] == selection]\n",
    "    if datasource_selection != \"__all__\":\n",
    "        filtered = filtered[filtered[\"OpenAIRE_DataSource_ID\"] == datasource_selection]\n",
    "    filtered = filtered.copy()\n",
    "\n",
    "    summary_widget.value = format_summary(filtered)\n",
    "\n",
    "    with output:\n",
    "        output.clear_output(wait=True)\n",
    "        if filtered.empty:\n",
    "            display(HTML(\"<p>No datasources match the current selection.</p>\"))\n",
    "            return\n",
    "\n",
    "        org_chart_df = (\n",
    "            filtered.groupby(\"Organisation Name\", as_index=False)[\"Total Research Products\"]\n",
    "            .sum()\n",
    "            .rename(columns={\"Total Research Products\": \"Total Research Products by Affiliation\"})\n",
    "            .sort_values(\"Total Research Products by Affiliation\", ascending=False)\n",
    "        )\n",
    "\n",
    "        totals_map = dict(zip(org_chart_df[\"Organisation Name\"], org_chart_df[\"Total Research Products by Affiliation\"]))\n",
    "        filtered[\"Total Research Products by Affiliation\"] = filtered[\"Organisation Name\"].map(totals_map)\n",
    "\n",
    "        volumetric = filtered.dropna(subset=value_cols, how=\"all\").copy()\n",
    "        volumetric = volumetric.sort_values(\"Total Research Products\", ascending=False)\n",
    "\n",
    "        shared_candidates = []\n",
    "        if not org_chart_df.empty:\n",
    "            shared_candidates.append(org_chart_df[\"Total Research Products by Affiliation\"].max())\n",
    "        if not volumetric.empty:\n",
    "            shared_candidates.append(volumetric[\"Total Research Products\"].max())\n",
    "        shared_candidates = [value for value in shared_candidates if pd.notna(value)]\n",
    "        axis_max = max(shared_candidates) * 1.1 if shared_candidates else None\n",
    "\n",
    "        if not org_chart_df.empty:\n",
    "            fig_org = px.bar(\n",
    "                org_chart_df,\n",
    "                x=\"Total Research Products by Affiliation\",\n",
    "                y=\"Organisation Name\",\n",
    "                title=\"Research products by affiliated organisation\",\n",
    "                orientation=\"h\",\n",
    "                height=max(400, 25 * len(org_chart_df)),\n",
    "            )\n",
    "            if axis_max:\n",
    "                fig_org.update_xaxes(range=[0, axis_max])\n",
    "            fig_org.update_layout(yaxis_title=\"Organisation\", legend=dict(orientation=\"h\", y=1.02, x=0))\n",
    "            display(fig_org)\n",
    "        else:\n",
    "            display(HTML(\"<p>No organisation totals available for the current selection.</p>\"))\n",
    "\n",
    "        if not volumetric.empty:\n",
    "            fig_products = px.bar(\n",
    "                volumetric,\n",
    "                x=\"Total Research Products\",\n",
    "                y=\"Datasource Name\",\n",
    "                color=\"Type\",\n",
    "                title=f\"Datasource volume snapshot (date: {latest_date})\",\n",
    "                orientation=\"h\",\n",
    "                height=max(400, 30 * len(volumetric)),\n",
    "                hover_data={\"Organisation Name\": True},\n",
    "            )\n",
    "            if axis_max:\n",
    "                fig_products.update_xaxes(range=[0, axis_max])\n",
    "            fig_products.update_layout(\n",
    "                yaxis_title=\"Datasource\",\n",
    "                legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"left\", x=0)\n",
    "            )\n",
    "            display(fig_products)\n",
    "        else:\n",
    "            display(HTML(\"<p>No datasource volume data available.</p>\"))\n",
    "\n",
    "        support_counts = {\n",
    "            \"CERIF (CRIS)\": filtered[\"detected_support_oai_cerif_openaire\"].sum(),\n",
    "            \"OpenAIRE Literature\": filtered[\"detected_support_oai_openaire\"].sum(),\n",
    "            \"OpenAIRE Data\": filtered[\"detected_support_openaire_data\"].sum(),\n",
    "            \"NL DIDL\": filtered[\"detected_support_nl_didl\"].sum(),\n",
    "            \"RIOXX\": filtered[\"detected_support_rioxx\"].sum(),\n",
    "        }\n",
    "        support_df = pd.DataFrame(\n",
    "            {\"Requirement\": list(support_counts.keys()), \"Endpoints\": list(support_counts.values())}\n",
    "        )\n",
    "        fig_support = px.bar(\n",
    "            support_df,\n",
    "            x=\"Requirement\",\n",
    "            y=\"Endpoints\",\n",
    "            title=\"Metadata-format support\"\n",
    "        )\n",
    "        fig_support.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"left\", x=0))\n",
    "        display(fig_support)\n",
    "\n",
    "        compat_counts = (\n",
    "            filtered.groupby(\"OpenAIRE Compatibility\", dropna=False)[\"OpenAIRE_DataSource_ID\"]\n",
    "            .nunique()\n",
    "            .reset_index(name=\"Datasources\")\n",
    "            .sort_values(\"Datasources\", ascending=False)\n",
    "        )\n",
    "        if not compat_counts.empty:\n",
    "            fig_compat = px.bar(\n",
    "                compat_counts,\n",
    "                x=\"Datasources\",\n",
    "                y=\"OpenAIRE Compatibility\",\n",
    "                title=\"Datasources by OpenAIRE compatibility\",\n",
    "                orientation=\"h\",\n",
    "            )\n",
    "            fig_compat.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"left\", x=0))\n",
    "            display(fig_compat)\n",
    "\n",
    "        type_compat_df = filtered.copy()\n",
    "        type_compat_df[\"Type\"] = type_compat_df[\"Type\"].fillna(\"Unknown\")\n",
    "        type_counts = (\n",
    "            type_compat_df.groupby([\"Type\", \"OpenAIRE Compatibility\"], dropna=False)[\"OpenAIRE_DataSource_ID\"]\n",
    "            .nunique()\n",
    "            .reset_index(name=\"Datasources\")\n",
    "        )\n",
    "        if not type_counts.empty:\n",
    "            type_pivot = type_counts.pivot(index=\"Type\", columns=\"OpenAIRE Compatibility\", values=\"Datasources\").fillna(0)\n",
    "            fig_type_heatmap = go.Figure(\n",
    "                data=go.Heatmap(\n",
    "                    z=type_pivot.values,\n",
    "                    x=type_pivot.columns,\n",
    "                    y=type_pivot.index,\n",
    "                    colorscale=\"Blues\",\n",
    "                    colorbar=dict(title=\"Datasources\")\n",
    "                )\n",
    "            )\n",
    "            fig_type_heatmap.update_layout(title=\"Datasource types vs. OpenAIRE compatibility\", xaxis_title=\"OpenAIRE compatibility\", yaxis_title=\"Datasource type\")\n",
    "            display(fig_type_heatmap)\n",
    "\n",
    "        support_mapping = {\n",
    "            \"OpenAIRE CERIF\": \"detected_support_oai_cerif_openaire\",\n",
    "            \"OpenAIRE Literature\": \"detected_support_oai_openaire\",\n",
    "            \"OpenAIRE Data\": \"detected_support_openaire_data\",\n",
    "            \"NL DIDL\": \"detected_support_nl_didl\",\n",
    "            \"RIOXX\": \"detected_support_rioxx\",\n",
    "        }\n",
    "        support_records = []\n",
    "        for label, column in support_mapping.items():\n",
    "            mask = filtered[column]\n",
    "            if mask.any():\n",
    "                compat_series = filtered.loc[mask, \"OpenAIRE Compatibility\"].fillna(\"Not specified\")\n",
    "                support_records.extend([(label, compat) for compat in compat_series])\n",
    "        if support_records:\n",
    "            support_heatmap_df = (\n",
    "                pd.DataFrame(support_records, columns=[\"Support\", \"OpenAIRE Compatibility\"])\n",
    "                .groupby([\"Support\", \"OpenAIRE Compatibility\"], dropna=False)\n",
    "                .size()\n",
    "                .reset_index(name=\"Datasources\")\n",
    "            )\n",
    "            support_pivot = support_heatmap_df.pivot(index=\"Support\", columns=\"OpenAIRE Compatibility\", values=\"Datasources\").fillna(0)\n",
    "            fig_support_heatmap = go.Figure(\n",
    "                data=go.Heatmap(\n",
    "                    z=support_pivot.values,\n",
    "                    x=support_pivot.columns,\n",
    "                    y=support_pivot.index,\n",
    "                    colorscale=\"Purples\",\n",
    "                    colorbar=dict(title=\"Datasources\")\n",
    "                )\n",
    "            )\n",
    "            fig_support_heatmap.update_layout(title=\"Supported endpoint types vs. OpenAIRE compatibility\", xaxis_title=\"OpenAIRE compatibility\", yaxis_title=\"Endpoint support type\")\n",
    "            display(fig_support_heatmap)\n",
    "\n",
    "        def make_org_link(row):\n",
    "            name = row[\"Organisation Name\"]\n",
    "            org_id = row.get(\"OpenAIRE_ORG_ID\")\n",
    "            if pd.isna(org_id) or not str(org_id).strip():\n",
    "                return html.escape(name)\n",
    "            escaped_name = html.escape(name)\n",
    "            escaped_id = html.escape(str(org_id).strip(), quote=True)\n",
    "            return f\"<a href='{OPENAIRE_ORG_URL}{escaped_id}' target='_blank' rel='noopener noreferrer'>{escaped_name}</a>\"\n",
    "\n",
    "        def make_datasource_link(row):\n",
    "            name = row[\"Datasource Name\"]\n",
    "            source_id = row.get(\"OpenAIRE_DataSource_ID\")\n",
    "            if pd.isna(source_id) or not str(source_id).strip():\n",
    "                return html.escape(name)\n",
    "            escaped_name = html.escape(name)\n",
    "            escaped_id = html.escape(str(source_id).strip(), quote=True)\n",
    "            return f\"<a href='{OPENAIRE_DATASOURCE_URL}{escaped_id}' target='_blank' rel='noopener noreferrer'>{escaped_name}</a>\"\n",
    "\n",
    "        def make_endpoint_link(url) -> str:\n",
    "            if pd.isna(url):\n",
    "                return \"\"\n",
    "            base_url = str(url).strip()\n",
    "            if not base_url:\n",
    "                return \"\"\n",
    "            identify_url = append_identify_param(base_url)\n",
    "            if not identify_url:\n",
    "                return \"\"\n",
    "            escaped_url = html.escape(base_url)\n",
    "            escaped_identify = html.escape(identify_url, quote=True)\n",
    "            return f\"<a href='{escaped_identify}' target='_blank' rel='noopener noreferrer'>{escaped_url}</a>\"\n",
    "\n",
    "        filtered_display = filtered.sort_values(\"Organisation Name\").copy()\n",
    "        filtered_display[\"Organisation Name\"] = filtered_display.apply(make_org_link, axis=1)\n",
    "        filtered_display[\"Datasource Name\"] = filtered_display.apply(make_datasource_link, axis=1)\n",
    "        filtered_display[\"OAI-endpoint\"] = filtered_display[\"OAI-endpoint\"].apply(make_endpoint_link)\n",
    "\n",
    "        table_cols = [\n",
    "            \"Organisation Name\",\n",
    "            \"Total Research Products by Affiliation\",\n",
    "            \"Datasource Name\",\n",
    "            \"OpenAIRE Compatibility\",\n",
    "            \"Type\",\n",
    "            \"OAI-endpoint\",\n",
    "            \"oai_status\",\n",
    "            \"metadata_prefixes_detected\",\n",
    "            \"Total Research Products\",\n",
    "            \"Publications\",\n",
    "            \"Research data\",\n",
    "            \"Research software\",\n",
    "            \"Other research products\",\n",
    "        ]\n",
    "        display(HTML(filtered_display[table_cols].to_html(index=False, escape=False)))\n",
    "\n",
    "def handle_org_change(change):\n",
    "    update_datasource_options()\n",
    "    render_dashboard()\n",
    "\n",
    "def handle_datasource_change(change):\n",
    "    if suppress_datasource_event:\n",
    "        return\n",
    "    render_dashboard()\n",
    "\n",
    "update_datasource_options()\n",
    "dropdown.observe(handle_org_change, names=\"value\")\n",
    "datasource_dropdown.observe(handle_datasource_change, names=\"value\")\n",
    "\n",
    "controls = widgets.HBox([dropdown, datasource_dropdown])\n",
    "\n",
    "display(HTML(\"<h2>Dutch Sources Monitor</h2>\"))\n",
    "display(summary_widget)\n",
    "display(controls)\n",
    "display(output)\n",
    "render_dashboard()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
